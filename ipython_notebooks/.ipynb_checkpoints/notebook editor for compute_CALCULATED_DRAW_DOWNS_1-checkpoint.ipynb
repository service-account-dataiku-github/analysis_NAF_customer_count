{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-env_clc",
      "display_name": "Python (env env_clc)",
      "language": "python"
    },
    "associatedRecipe": "compute_CALCULATED_DRAW_DOWNS",
    "dkuGit": {
      "lastInteraction": 0
    },
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "Daniel.Vandermeer"
      },
      "lastModifiedOn": 1667062120007
    },
    "creator": "Daniel.Vandermeer",
    "createdOn": 1667062120007,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {},
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "Daniel.Vandermeer"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\nfrom dateutil.relativedelta import relativedelta\nfrom helper import *\n\n# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\nimport pickle\nfrom dateutil.relativedelta import relativedelta\nimport gc\nfrom re import finditer\n\n## Find DD DU\nfrom helper import preprocess_data\nfrom patterns import find_drawdowns, find_drawups\n\n## MATCHING\nimport name_matching\nfrom name_matching import name_match\nimport transaction_matching\nfrom transaction_matching import transaction_match\n\n## CONSOLIDATION\nfrom consolidation import combine_matches, consolidate_matches, find_attritions, find_new_accounts, get_attrition_status, get_new_account_status"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "start_date \u003d dataiku.get_custom_variables()[\u0027start_date\u0027]\nend_date \u003d dataiku.get_custom_variables()[\u0027end_date\u0027]\n\nconsistency \u003d int(dataiku.get_custom_variables()[\u0027consistency\u0027])\ndrawdown_period_average \u003d int(dataiku.get_custom_variables()[\u0027drawdown_period_average\u0027])\ndrawdown \u003d int(dataiku.get_custom_variables()[\u0027drawdown\u0027])\ndrawdown_fwd_check \u003d int(dataiku.get_custom_variables()[\u0027drawdown_fwd_check\u0027])\ndrawdown_lookback_period \u003d int(dataiku.get_custom_variables()[\u0027drawdown_lookback_period\u0027])\ndrawup_lookfwd_period \u003d int(dataiku.get_custom_variables()[\u0027drawup_lookfwd_period\u0027])\nstatistics_period \u003d int(dataiku.get_custom_variables()[\u0027statistics_period\u0027])\ninactive_period \u003d int(dataiku.get_custom_variables()[\u0027inactive_period\u0027])\n\n## MATCHING VARIABLES\nmonth_diff_h \u003d int(dataiku.get_custom_variables()[\u0027month_diff_h\u0027])\nmonth_diff_l \u003d int(dataiku.get_custom_variables()[\u0027month_diff_l\u0027])\nsd_mul \u003d int(dataiku.get_custom_variables()[\u0027sd_mul\u0027])\nmax_city_distance \u003d int(dataiku.get_custom_variables()[\u0027max_city_distance\u0027])\nthreshold_score_step1 \u003d int(dataiku.get_custom_variables()[\u0027threshold_score_step1\u0027])\nthreshold_score_step2 \u003d int(dataiku.get_custom_variables()[\u0027threshold_score_step2\u0027])\n\n## RUN TYPE\nrun \u003d dataiku.get_custom_variables()[\u0027run_type\u0027]\n\nprint(\"start_date\", start_date)\nprint(\"end_date\", end_date)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read recipe inputs\nNAFCUSTOMER_ACTIVE_CARDS_FULL \u003d dataiku.Dataset(\"NAFCUSTOMER_ACTIVE_CARDS_FULL\")\nNAFCUSTOMER_ACTIVE_CARDS_FULL_df \u003d NAFCUSTOMER_ACTIVE_CARDS_FULL.get_dataframe()\nprint(len(NAFCUSTOMER_ACTIVE_CARDS_FULL_df))\nNAFCUSTOMER_ACTIVE_CARDS_FULL_df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def date_tz_naive(pd_s):\n    return pd.to_datetime(pd_s).apply(lambda x:x.tz_localize(None))\n\n# Read recipe inputs\nNAFCUSTOMER_ACTIVE_CARDS_FULL \u003d dataiku.Dataset(\"NAFCUSTOMER_ACTIVE_CARDS_FULL\")\nNAFCUSTOMER_ACTIVE_CARDS_FULL_df \u003d NAFCUSTOMER_ACTIVE_CARDS_FULL.get_dataframe()\n\nprint(len(NAFCUSTOMER_ACTIVE_CARDS_FULL_df))\nNAFCUSTOMER_ACTIVE_CARDS_FULL_df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 25,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "customer_list_full \u003d NAFCUSTOMER_ACTIVE_CARDS_FULL_df.CUSTOMER.unique()\nprint(len(customer_list_full))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "624069\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 37,
      "cell_type": "code",
      "metadata": {
        "scrolled": false
      },
      "source": [
        "page_size \u003d 100000\nidx \u003d 0\ncurrent_page \u003d 0\nmax_pages \u003d 0\n\nwhile idx\u003clen(customer_list_full):\n    \n    current_page+\u003d1\n    \n    to_range \u003d idx+page_size\n    if to_range\u003elen(customer_list_full):\n        to_range \u003d len(customer_list_full)-1\n    \n    current_set \u003d customer_list_full[idx:to_range]\n    print(len(current_set))\n    print(current_set[0])\n    print(idx)\n    print(len(customer_list_full)-idx, \"remaining\")\n    print()\n    \n    idx+\u003dpage_size\n    \n    if max_pages\u003e0:\n        if current_page\u003emax_pages:\n            break;\n            \nprint(\"done\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "100000\nENCORE DERMATOLOGY INC\n0\n624069 remaining\n\n100000\nTTG ELECTRIC CO INC\n100000\n524069 remaining\n\n100000\nABC RENTALS INC\n200000\n424069 remaining\n\n100000\nBAYSIDE MOTORSPORTS AND CYCLE LLC\n300000\n324069 remaining\n\n100000\nWALKER PAINTING SPECIALIST\n400000\n224069 remaining\n\n100000\nORANGE CREST FLOORING GROUP\n500000\n124069 remaining\n\n24068\nCARLSON CONSULTING ENGINEERS INC\n600000\n24069 remaining\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "customer_list \u003d [\u0027NORTHSHORE INC\u0027,\u0027JAYHAWK MILLWRIGHT\u0027,\u0027PAXTON ASSOCIATES INC\u0027,\u0027STATE OF NEW YORK\u0027,\n                \u0027PAXTON ASSOCIATES INC\u0027, \u0027YNGRID COSMETICZ LLC\u0027, \u0027CYRGUS COMPANY INC.\u0027, \u0027THE HEALTHY STOP INC\u0027,\n                 \u0027T F WALZ INC\u0027, \u0027JAYHAWK MILLWRIGHT\u0027, \u0027CREDIT SLAYERS LLC\u0027, \u0027A ABLE MOVING CO\u0027, \u0027STUDIO IMPACT INC\u0027]\n\ndf_v \u003d NAFCUSTOMER_ACTIVE_CARDS_FULL_df[NAFCUSTOMER_ACTIVE_CARDS_FULL_df.CUSTOMER.isin(customer_list)]\nprint(len(df_v.CUSTOMER.unique()), \"unique customers\")\nprint(len(df_v), \"records\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(len(df_v))\ndf_v[\u0027REVENUE_DATE\u0027] \u003d date_tz_naive(df_v[\u0027REVENUE_DATE\u0027])\nprint(len(df_v))\n\ndf_v \u003d df_v[[\u0027CUSTOMER\u0027,\u0027REVENUE_DATE\u0027, \u0027ACTIVE_CARD_COUNT\u0027]]\n\ndf_v_max \u003d df_v[[\u0027CUSTOMER\u0027,\u0027ACTIVE_CARD_COUNT\u0027]]\ndf_max \u003d df_v_max.groupby(by\u003d[\"CUSTOMER\"]).max().reset_index()\ndf_max.columns \u003d [\u0027CUSTOMER\u0027, \u0027ACTIVE_CARD_MAX\u0027]\n\nprint(len(df_v))\ndf_v.dropna(subset\u003d[\u0027CUSTOMER\u0027], inplace\u003dTrue)\nprint(len(df_v))\n\ndf_v[\u0027REVENUE_DATE\u0027] \u003d pd.to_datetime(df_v[\u0027REVENUE_DATE\u0027])\ndf_v.sort_values([\u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n\nseen_accounts \u003d df_v[df_v[\u0027ACTIVE_CARD_COUNT\u0027] \u003e 0].groupby([\u0027CUSTOMER\u0027], as_index\u003dFalse)[[\u0027REVENUE_DATE\u0027]].first()\nseen_accounts[\u0027FIRST_DATE\u0027] \u003d seen_accounts[\u0027REVENUE_DATE\u0027] - pd.DateOffset(months\u003d1)\n\ndf \u003d df_v\nperiod_end_date \u003d end_date\nmatch_type \u003d \u0027program_flip\u0027\nperiod_start_date\u003dNone\nsplit\u003dNone\n\ndrawdown \u003d (100 - drawdown)/100\ndrawdown_fwd_check /\u003d 100\n\ninactive_date_start \u003d pd.to_datetime(period_end_date) + relativedelta(months\u003d-inactive_period)\n\ndf \u003d df[df[\u0027REVENUE_DATE\u0027] \u003c\u003d period_end_date].copy()\n\nif period_start_date:\n    period_start_date \u003d pd.to_datetime(period_start_date)\n    df \u003d df[df[\u0027REVENUE_DATE\u0027] \u003e\u003d period_start_date].copy()\n\nall_customer_names \u003d list(df[\u0027CUSTOMER\u0027].unique())\n\nif not split:\n    split\u003d1\n\nall_customer_names_n \u003d list(split_list(all_customer_names, split))\n\ndrop_df \u003d pd.DataFrame()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "all_customer_names_n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def find_consistent_cust(df, consecutive\u003d3):\n    \u0027\u0027\u0027returns a list of customers who are consistent for 3 (default value) months\u0027\u0027\u0027\n\n    ## Needs only these columns [\u0027customer_account_name\u0027, \u0027revenue_month\u0027, \u0027purchase_gallons_qty\u0027]\n\n    df \u003d df[[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027, \u0027ACTIVE_CARD_COUNT\u0027]].copy()\n    df.sort_values(by\u003d[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n\n    z \u003d (df.groupby([\u0027CUSTOMER\u0027])[\u0027REVENUE_DATE\u0027].diff(1)/np.timedelta64(1, \u0027M\u0027))\n    z \u003d z.round(0)\n    z \u003d (z \u003d\u003d 1).astype(\u0027int\u0027)\n    df[\u0027CUST_CONS\u0027] \u003d (z * (z.groupby((z !\u003d z.shift()).cumsum()).cumcount() + 2))\n    cust_cons \u003d df.groupby(\u0027CUSTOMER\u0027)[\u0027CUST_CONS\u0027].max()\n\n    return list(cust_cons[cust_cons\u003e\u003dconsecutive].index)\n\ndef add_padding_func(df, padding\u003d12, last_date\u003dNone):\n    \u0027\u0027\u0027\n    Fills all the zeros in between for intermittent data and also fills the trailing data with\n    12 zeros or till the last date whichever is earlier\n    \u0027\u0027\u0027\n\n    cols \u003d [\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027]\n\n    common_cols \u003d set(df.columns).intersection(set(cols))\n\n    profile \u003d df[common_cols].drop_duplicates()\n\n    vol \u003d df[[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027, \u0027ACTIVE_CARD_COUNT\u0027]].copy()\n    vol \u003d vol.groupby([\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027])[[\u0027ACTIVE_CARD_COUNT\u0027]].sum().reset_index()\n    vol.reset_index(drop\u003dTrue, inplace\u003dTrue)\n\n    vol.sort_values([\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n    vol.reset_index(drop\u003dTrue, inplace\u003dTrue)\n\n    last_rev_date \u003d vol.groupby([\u0027CUSTOMER\u0027])[[\u0027REVENUE_DATE\u0027]].last()\n    last_rev_date \u003d last_rev_date[last_rev_date[\u0027REVENUE_DATE\u0027] \u003c pd.to_datetime(last_date)]\n    last_rev_date[\u0027REVENUE_DATE\u0027] \u003d last_rev_date[\u0027REVENUE_DATE\u0027] + pd.DateOffset(months\u003dpadding)\n    last_rev_date[\u0027LAST_DATE\u0027] \u003d pd.to_datetime(last_date)\n    last_rev_date[\u0027REVENUE_DATE\u0027] \u003d last_rev_date[[\u0027REVENUE_DATE\u0027,\u0027LAST_DATE\u0027]].min(axis\u003d1)\n    last_rev_date.drop([\u0027LAST_DATE\u0027], axis\u003d1, inplace\u003dTrue)\n    last_rev_date.reset_index(inplace\u003dTrue)\n    vol \u003d pd.concat([vol, last_rev_date], ignore_index\u003dTrue)\n    vol.fillna(0, inplace\u003dTrue)\n    vol \u003d (vol.set_index(\u0027REVENUE_DATE\u0027).groupby(\u0027CUSTOMER\u0027).resample(\u0027MS\u0027).asfreq()\n                  .drop([\u0027CUSTOMER\u0027], 1).reset_index())\n    vol.fillna(0, inplace\u003dTrue)\n    df \u003d vol.merge(profile, how\u003d\u0027left\u0027, on \u003d [\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027])\n    df.fillna(method\u003d\u0027ffill\u0027, inplace\u003dTrue)\n\n    return df\n\ndef find_average_func(dd_find, n\u003d12):\n\n    dd_find.sort_values([\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n    dd_find2 \u003d dd_find.sort_values([\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027], ascending\u003d[True, False]).reset_index(drop\u003dTrue)\n\n    dd_find.reset_index(drop\u003dTrue, inplace\u003dTrue)\n    dd_find[\u0027LAST_N_MONTHS_AVG\u0027] \u003d dd_find.groupby([\u0027CUSTOMER\u0027])[\u0027ACTIVE_CARD_COUNT\u0027]\\\n                                        .rolling(n, min_periods\u003d1).mean().reset_index(drop\u003dTrue)\n    dd_find2[\u0027NEXT_N_MONTHS_AVG\u0027] \u003d dd_find2.groupby([\u0027CUSTOMER\u0027])[\u0027ACTIVE_CARD_COUNT\u0027]\\\n                                        .rolling(n, min_periods\u003d1).mean().reset_index(drop\u003dTrue)\n\n    dd_find[\u0027LAST_N_MONTHS_AVG\u0027] \u003d dd_find.groupby(\u0027CUSTOMER\u0027)[\u0027LAST_N_MONTHS_AVG\u0027].shift(1)\n    dd_find2[\u0027NEXT_N_MONTHS_AVG\u0027] \u003d dd_find2.groupby(\u0027CUSTOMER\u0027)[\u0027NEXT_N_MONTHS_AVG\u0027].shift(1)\n\n    dd_find \u003d dd_find.merge(dd_find2[[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027, \u0027NEXT_N_MONTHS_AVG\u0027]],\n                on\u003d[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027])\n\n    return dd_find"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "idx \u003d 0\nmax_idx \u003d 5\n\nfor sublist in all_customer_names_n:\n\n    idx+\u003d1\n\n    print(sublist)\n\n    dd_find \u003d df[df[\u0027CUSTOMER\u0027].isin(sublist)].copy()\n\n    consistent_customers_dd \u003d find_consistent_cust(dd_find, consecutive\u003dconsistency)\n    if len(consistent_customers_dd) \u003d\u003d 0:\n        continue\n\n    dd_find \u003d dd_find[dd_find[\u0027CUSTOMER\u0027].isin(consistent_customers_dd)].copy()\n    dd_find \u003d add_padding_func(dd_find, padding\u003dstatistics_period, last_date\u003dperiod_end_date)\n    dd_find \u003d find_average_func(dd_find, n\u003dstatistics_period)\n\n    dd_find[\u0027DD_INDICATOR\u0027] \u003d np.where(((drawdown*(dd_find[\u0027LAST_N_MONTHS_AVG\u0027].round(3)) \u003e\n                                     dd_find[\u0027ACTIVE_CARD_COUNT\u0027].round(3)) \u0026\n                                    (dd_find[\u0027NEXT_N_MONTHS_AVG\u0027].round(3) \u003c\n                                     drawdown_fwd_check*dd_find[\u0027LAST_N_MONTHS_AVG\u0027].round(3))),\n                                   True, False)\n\n    ## Find the first drawdown and also the list of customers\n    pflip_dd \u003d dd_find[dd_find[\u0027DD_INDICATOR\u0027] \u003d\u003d True].copy()\n    pflip_dd.drop_duplicates(\u0027CUSTOMER\u0027, inplace\u003dTrue)\n    first_drop_idx \u003d pflip_dd.index\n    pflip_dd_customers \u003d list(dd_find[\u0027CUSTOMER\u0027].unique())\n    first_drop \u003d dd_find.iloc[first_drop_idx]\n\n    ## Identify the lookback period\n    first_drop \u003d first_drop[[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027]].copy()\n    first_drop \u003d first_drop[first_drop[\u0027REVENUE_DATE\u0027] \u003c\u003d inactive_date_start].copy()\n    first_drop[\u0027START_DATE\u0027]  \u003d first_drop[\u0027REVENUE_DATE\u0027] - pd.DateOffset(months\u003ddrawdown_lookback_period)\n    first_drop.rename(columns \u003d {\u0027REVENUE_DATE\u0027:\u0027DD_DATE\u0027}, inplace\u003dTrue)\n    dd_find_df \u003d dd_find[dd_find[\u0027CUSTOMER\u0027].isin(pflip_dd_customers)]\n    dd_find_df \u003d dd_find_df.merge(first_drop, on\u003d[\u0027CUSTOMER\u0027])\n    dd_find_df \u003d dd_find_df[dd_find_df[\u0027REVENUE_DATE\u0027].between(dd_find_df[\u0027START_DATE\u0027],dd_find_df[\u0027DD_DATE\u0027])].copy()\n\n    ## Compute the sharpest fall from the lookback period\n    dd_find_df.sort_values([\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n    dd_find_df[\u0027DROP\u0027] \u003d dd_find_df.groupby([\u0027CUSTOMER\u0027])[\u0027ACTIVE_CARD_COUNT\u0027].diff(-1)\n\n    ## Find the corresponding period and remove duplicates in case of a similar values\n    drop_idx \u003d dd_find_df.groupby([\u0027CUSTOMER\u0027])[\u0027DROP\u0027].transform(max) \u003d\u003d dd_find_df[\u0027DROP\u0027]\n    drop_month_df \u003d dd_find_df[drop_idx].copy()\n    drop_month_df.drop_duplicates([\u0027CUSTOMER\u0027], inplace\u003dTrue)\n\n    ## remove the first record\n    dd_find \u003d dd_find.groupby(\u0027CUSTOMER\u0027).apply(lambda group: group.iloc[1:, 1:]).reset_index()\n    dd_find.drop(\u0027level_1\u0027, axis\u003d1, inplace\u003dTrue)\n\n    ## Find the time periods for calculating statistics (mean and standard deviation)\n    drop_month_df.rename(columns \u003d {\u0027REVENUE_DATE\u0027:\u0027DROP_DATE\u0027}, inplace\u003dTrue)\n    dd_find \u003d dd_find.merge(drop_month_df[[\u0027CUSTOMER\u0027, \u0027DROP_DATE\u0027]], on\u003d\u0027CUSTOMER\u0027)\n    dd_find[\u0027END_DATE\u0027] \u003d dd_find[\u0027DROP_DATE\u0027] - pd.DateOffset(months\u003d3)\n    dd_find[\u0027START_DATE\u0027] \u003d dd_find[\u0027END_DATE\u0027] - pd.DateOffset(months\u003dstatistics_period-1)\n    pflip_12_data \u003d dd_find[dd_find[\u0027REVENUE_DATE\u0027].between(dd_find[\u0027START_DATE\u0027], dd_find[\u0027END_DATE\u0027])].copy()\n\n    ## Calculate Mean and Standard Deviation\n    dd_stat \u003d pflip_12_data.groupby([\u0027CUSTOMER\u0027], as_index\u003dFalse).agg({\u0027ACTIVE_CARD_COUNT\u0027:[\u0027mean\u0027,\u0027std\u0027]})\n    dd_stat.columns \u003d [\u0027CUSTOMER_DD\u0027, \u0027MEAN_DD\u0027,\u0027STD_DD\u0027]\n    drop_month_df \u003d drop_month_df.merge(dd_stat,\n                                        left_on\u003d\u0027CUSTOMER\u0027,\n                                        right_on\u003d\u0027CUSTOMER_DD\u0027,\n                                        how\u003d\u0027left\u0027)\n\n    drop_df \u003d pd.concat([drop_df, drop_month_df], ignore_index\u003dTrue)\n\n    if(idx\u003emax_idx):\n        break;\n\ndrop_df.drop([\u0027CUSTOMER_DD\u0027], axis\u003d1, inplace\u003dTrue)\ndrop_df.rename(columns\u003d{\u0027DROP_DATE\u0027:\u0027DRAW_DOWN_DATE\u0027,\n                        \u0027DROP\u0027:\u0027DROP_QTY\u0027}, inplace\u003dTrue)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "drop_df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(len(drop_df))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute recipe outputs from inputs\n# TODO: Replace this part by your actual code that computes the output, as a Pandas dataframe\n# NB: DSS also supports other kinds of APIs for reading and writing data. Please see doc.\n\nCALCULATED_DRAW_DOWNS_df \u003d drop_df\n\n# Write recipe outputs\nCALCULATED_DRAW_DOWNS \u003d dataiku.Dataset(\"CALCULATED_DRAW_DOWNS\")\nCALCULATED_DRAW_DOWNS.write_with_schema(CALCULATED_DRAW_DOWNS_df)"
      ],
      "outputs": []
    }
  ]
}