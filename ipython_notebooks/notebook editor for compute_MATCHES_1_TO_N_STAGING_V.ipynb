{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-env_clc",
      "display_name": "Python (env env_clc)",
      "language": "python"
    },
    "associatedRecipe": "compute_MATCHES_1_TO_N_STAGING_V",
    "dkuGit": {
      "lastInteraction": 0
    },
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "Daniel.Vandermeer"
      },
      "lastModifiedOn": 1667169923254
    },
    "creator": "Daniel.Vandermeer",
    "createdOn": 1667169923254,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {},
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "Daniel.Vandermeer"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\n# Read recipe inputs\nCALCULATED_DRAW_DOWNS \u003d dataiku.Dataset(\"CALCULATED_DRAW_DOWNS\")\nCALCULATED_DRAW_DOWNS_df \u003d CALCULATED_DRAW_DOWNS.get_dataframe()\n\nCALCULATED_DRAW_UPS \u003d dataiku.Dataset(\"CALCULATED_DRAW_UPS\")\nCALCULATED_DRAW_UPS_df \u003d CALCULATED_DRAW_UPS.get_dataframe()\n\nCOMMON_WORDS \u003d dataiku.Dataset(\"NAFCUSTOMER_COMMON_WORDS_IN_NAMES\")\nCOMMON_WORDS_df \u003d COMMON_WORDS.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 4,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def date_tz_naive(pd_s):\n    return pd.to_datetime(pd_s).apply(lambda x:x.tz_localize(None))"
      ],
      "outputs": []
    },
    {
      "execution_count": 7,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_down_full \u003d CALCULATED_DRAW_DOWNS_df\ndf_up_full \u003d CALCULATED_DRAW_UPS_df\ndf_common \u003d COMMON_WORDS_df\n\ndf_down_full.DRAW_DOWN_DATE \u003d date_tz_naive(df_down_full[\u0027DRAW_DOWN_DATE\u0027])\ndf_up_full.DRAW_UP_DATE \u003d date_tz_naive(df_up_full[\u0027DRAW_UP_DATE\u0027])\n\ndf_down_full.sort_values([\u0027CUSTOMER\u0027], inplace\u003dTrue)\ndf_up_full.sort_values([\u0027CUSTOMER\u0027], inplace\u003dTrue)\ndf_common.sort_values([\u0027WORD\u0027], inplace\u003dTrue)\n\nprint(len(df_down_full), \"draw downs full\")\nprint(len(df_up_full), \"draw ups full\")\nprint(len(df_common), \"common words\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "88747 draw downs full\n330570 draw ups full\n2173 common words\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 11,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import string\n\n_common_words \u003d df_common.WORD.unique()\nprint(len(_common_words), \"screening against common words\")\n\nclass Draw_Down_Customer:\n\n    def __init__(self, name, draw_down_date, active_card_max):\n\n        self.CUSTOMER \u003d name\n        self.DRAW_DOWN_DATE \u003d draw_down_date\n        self.ACTIVE_CARD_MAX \u003d active_card_max\n\n        self.MATCHING_CUSTOMERS \u003d []\n        self.PERCENT_DIFFERENCE \u003d []\n        self.DAYS_DIFFERENCE \u003d []\n        self.DRAW_UP_DATE \u003d []\n\n        # remove punctuation\n        c_str \u003d name.translate(str.maketrans(\u0027\u0027, \u0027\u0027, string.punctuation))\n\n        f \u003d c_str.split()\n        self.WORD_LIST \u003d []\n        for w in f:\n            if w not in _common_words:\n                self.WORD_LIST.append(w)\n\n    def Match_Draw_Up_Customer(self, name, draw_up_date, active_card_max):\n\n        if (self.CUSTOMER \u003d\u003d name):\n            # exact match, already captured\n            return\n\n        c_str \u003d name.translate(str.maketrans(\u0027\u0027, \u0027\u0027, string.punctuation))\n\n        f \u003d c_str.split()\n\n        check_list \u003d []\n        for w in f:\n            if (w not in _common_words) and (len(w)\u003e1) and (not w.isnumeric()):\n                check_list.append(w)\n\n        percent_diff \u003d round((abs(self.ACTIVE_CARD_MAX - active_card_max) / ((self.ACTIVE_CARD_MAX+active_card_max)/2)),2)\n\n        #date_format \u003d \"%Y-%m-%d\"\n        #d1_date \u003d datetime.strptime(draw_up_date.astype(str), date_format)\n        #d2_date \u003d datetime.strptime(self.DRAW_DOWN_DATE.astype(str), date_format)\n\n        delta_between_drop_and_rise \u003d round(abs((draw_up_date-self.DRAW_DOWN_DATE).days)/30.,0)\n\n        for w_to_check in check_list:\n            for w in self.WORD_LIST:\n                if w_to_check\u003d\u003dw:\n\n                    if not name in(self.MATCHING_CUSTOMERS) and(delta_between_drop_and_rise\u003c\u003d4)and(percent_diff\u003c\u003d0.5) :\n                        self.MATCHING_CUSTOMERS.append(name)\n                        self.PERCENT_DIFFERENCE.append(percent_diff)\n                        self.DAYS_DIFFERENCE.append(delta_between_drop_and_rise)\n                        self.DRAW_UP_DATE.append(draw_up_date)\n                        break;"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "2173 screening against common words\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 12,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n\ndef do_save_log(_matching_process_log_time, _matching_process_log_event):\n\n    df_matching_log \u003d pd.DataFrame(_matching_process_log_time)\n    if len(df_matching_log)\u003e0:\n\n        df_matching_log.columns \u003d [\u0027LOG_TIME\u0027]\n        df_matching_log[\u0027LOG_EVENT\u0027] \u003d _matching_process_log_event\n\n        MATCHING_PROCESS_LOG_V_df \u003d df_matching_log\n        MATCHING_PROCESS_LOG_V \u003d dataiku.Dataset(\"MATCHING_PROCESS_LOG_V\")\n        MATCHING_PROCESS_LOG_V.write_with_schema(MATCHING_PROCESS_LOG_V_df)\n\n        print()\n\ndef do_save_direct_matches(_direct_customer, _direct_match, _direct_draw_up_date):\n\n    df_matches \u003d pd.DataFrame(_direct_customer)\n    if len(df_matches)\u003e0:\n\n        print()\n        print(\"saving\", len(df_matches), \"1-1 matching records\")\n        print()\n\n        df_matches.columns \u003d [\u0027CUSTOMER\u0027]\n        df_matches[\"MATCH_CUSTOMER\"] \u003d _direct_match\n        df_matches[\"DRAW_UP_DATE\"] \u003d _direct_draw_up_date\n\n        MATCHES_1_TO_1_STAGING_V_df \u003d df_matches\n        MATCHES_1_TO_1_STAGING_V \u003d dataiku.Dataset(\"MATCHES_1_TO_1_STAGING_V\")\n        MATCHES_1_TO_1_STAGING_V.write_with_schema(MATCHES_1_TO_1_STAGING_V_df)\n\n        print()\n\ndef do_save_multiple_matches(_multiple_customer, _multiple_matches, _multiple_drop_dates):\n\n    df_multiple_matches \u003d pd.DataFrame(_multiple_customer)\n\n    if len(df_multiple_matches)\u003e0:\n\n        print()\n        print(\"saving\", len(df_multiple_matches), \"1-n matching records\")\n        print()\n\n        df_multiple_matches.columns \u003d [\u0027CUSTOMER\u0027]\n        df_multiple_matches[\"MATCH_CUSTOMER\"] \u003d _multiple_matches\n        df_multiple_matches[\"DRAW_UP_DATE\"] \u003d _multiple_drop_dates\n\n        MATCHES_1_TO_N_STAGING_V_df \u003d df_multiple_matches\n        MATCHES_1_TO_N_STAGING_V \u003d dataiku.Dataset(\"MATCHES_1_TO_N_STAGING_V\")\n        MATCHES_1_TO_N_STAGING_V.write_with_schema(MATCHES_1_TO_N_STAGING_V_df)\n\n        print()"
      ],
      "outputs": []
    },
    {
      "execution_count": 14,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_down \u003d df_down_full\ndf_up \u003d df_up_full\n\n_processed_customers \u003d []\nverbose \u003d False\n\n_matching_process_log_time \u003d []\n_matching_process_log_event \u003d []\n\n_direct_customer \u003d []\n_direct_match \u003d []\n_direct_draw_up_date \u003d []\n\n_multiple_customer \u003d []\n_multiple_matches \u003d []\n_multiple_drop_dates \u003d []\n\n_no_match_customer \u003d []\n\nsave_every_n \u003d 50\nto_save_counter \u003d 0\nprint_every_n \u003d 25\n\nprint(len(df_down), \"filtered down rows\")\nprint(len(df_up), \"filtered up rows\")\n\n_customers \u003d []\n\nt0 \u003d time.time()\n\nfor index, row in df_down.iterrows():\n    \n    customer \u003d row[\u0027CUSTOMER\u0027]\n    draw_down_date \u003d row[\u0027DRAW_DOWN_DATE\u0027]\n    active_card_max \u003d row[\u0027ACTIVE_CARD_MAX\u0027]\n\n    c \u003d Draw_Down_Customer(customer, draw_down_date, active_card_max)\n\n    _customers.append(c)\n\nidx \u003d 0\n\n_matching_process_log_time.append(str(pd.Timestamp.now()))\n_matching_process_log_event.append(\" processing range \" + str(len(_customers)) + \" Draw Down Customers\")\ndo_save_log(_matching_process_log_time, _matching_process_log_event)\n\nmax_idx \u003d 5\n\n\n        \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "88747 filtered down rows\n330570 filtered up rows\n1 rows successfully written (le3WnXeH87)\n\n DANLEY ELEC CONTRACTING LLC 2019-05-01 00:00:00 17.0\n HOBURGS HOME HELPERS 2021-11-01 00:00:00 4.0\n HOLDINGS LLC DBA SECURITY INDUSTRIES 2019-08-01 00:00:00 10.0\n POINT READY MIX LLC 2022-01-01 00:00:00 1.0\n0009 CTV, INC 2020-01-01 00:00:00 1.0\n0067 BLACK \u0026 DECKER 2020-10-01 00:00:00 687.0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 34,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datetime import timedelta\n\nidx\u003d0\nfor c in _customers:\n    \n    idx+\u003d1\n    \n    #self.CUSTOMER \u003d name\n    #self.DRAW_DOWN_DATE \u003d draw_down_date\n    #self.ACTIVE_CARD_MAX \u003d active_card_max\n    print(c.CUSTOMER, c.DRAW_DOWN_DATE, c.ACTIVE_CARD_MAX)\n\n    dt_start \u003d pd.to_datetime(c.DRAW_DOWN_DATE) +timedelta(days\u003d-120)\n    dt_end \u003d pd.to_datetime(c.DRAW_DOWN_DATE) +timedelta(days\u003d120)\n    \n    card_delta \u003d c.ACTIVE_CARD_MAX * 0.5\n    card_start \u003d c.ACTIVE_CARD_MAX - card_delta\n    card_end \u003d c.ACTIVE_CARD_MAX + card_delta\n    \n    print(\"between\", dt_start, dt_end)\n    \n    df_up \u003d df_up_full[df_up_full[\u0027DRAW_UP_DATE\u0027]\u003c pd.to_datetime(c.DRAW_DOWN_DATE) +pd.Timedelta(\u0027120D\u0027)]\n    \n    print(len(df_up), len(df_up_full))\n    \n    #df_up \u003d df_up_full[df_up_full[\u0027DRAW_UP_DATE\u0027].between()]\n    \n    #df_v \u003d df_v[df_v[\u0027REVENUE_DATE\u0027].between(pd.to_datetime(start_date), pd.to_datetime(end_date))].copy()\n    \n    \n    if idx\u003emax_idx:\n        break;"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": " DANLEY ELEC CONTRACTING LLC 2019-05-01 00:00:00 17.0\nbetween 2019-01-01 00:00:00 2019-08-29 00:00:00\n0 330570\n HOBURGS HOME HELPERS 2021-11-01 00:00:00 4.0\nbetween 2021-07-04 00:00:00 2022-03-01 00:00:00\n0 330570\n HOLDINGS LLC DBA SECURITY INDUSTRIES 2019-08-01 00:00:00 10.0\nbetween 2019-04-03 00:00:00 2019-11-29 00:00:00\n0 330570\n POINT READY MIX LLC 2022-01-01 00:00:00 1.0\nbetween 2021-09-03 00:00:00 2022-05-01 00:00:00\n1413 330570\n0009 CTV, INC 2020-01-01 00:00:00 1.0\nbetween 2019-09-03 00:00:00 2020-04-30 00:00:00\n0 330570\n0067 BLACK \u0026 DECKER 2020-10-01 00:00:00 687.0\nbetween 2020-06-03 00:00:00 2021-01-29 00:00:00\n0 330570\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "_matching_process_log_time.append(str(pd.Timestamp.now()))\n_matching_process_log_event.append(\" processing range \" + str(len(_customers)) + \" Draw Down Customers\")\ndo_save_log(_matching_process_log_time, _matching_process_log_event)\n\nif verbose:\n    print(\" processing range from \" + str(len(_customers)) + \" Draw Down Customers\")\n\nfor c in _customers:\n\n    idx+\u003d1\n    \n    for index_up, row_up in df_up.iterrows():\n\n        customer \u003d row_up[\u0027CUSTOMER\u0027]\n        draw_up_date \u003d row_up[\u0027DRAW_UP_DATE\u0027]\n        mean_du \u003d row_up[\u0027MEAN_DU\u0027]\n        std_du \u003d row_up[\u0027STD_DU\u0027]\n        active_card_max \u003d row_up[\u0027ACTIVE_CARD_MAX\u0027]\n\n        c.Match_Draw_Up_Customer(customer, draw_up_date, mean_du, std_du, active_card_max)\n\n    if len(c.MATCHING_CUSTOMERS)\u003d\u003d1:\n\n        if not c.CUSTOMER in (_processed_customers):\n\n            to_save_counter +\u003d 1\n\n            _direct_customer.append(c.CUSTOMER)\n            _processed_customers.append(c.CUSTOMER)\n            _direct_match.append(c.MATCHING_CUSTOMERS[0])\n            _processed_customers.append(c.MATCHING_CUSTOMERS[0])\n            _direct_draw_up_date.append(c.DRAW_UP_DATE[0])\n\n            if verbose:\n                print()\n                print(\"DIRECT\")\n                print(c.CUSTOMER, c.WORD_LIST)\n                print(c.MATCHING_CUSTOMERS)\n                print(c.PERCENT_DIFFERENCE)\n                print(c.DAYS_DIFFERENCE)\n                print(\"\u003d\u003d\u003d\u003d\u003d\")\n                print()\n\n    elif len(c.MATCHING_CUSTOMERS)\u003e1:\n\n        if not c.CUSTOMER in (_processed_customers):\n\n            to_save_counter +\u003d 1\n\n            _multiple_customer.append(c.CUSTOMER)\n            _processed_customers.append(c.CUSTOMER)\n            _multiple_matches.append(c.MATCHING_CUSTOMERS)\n            _multiple_drop_dates.append(c.DRAW_UP_DATE)\n\n        if verbose:\n            print()\n            print(\"MULTIPLE\")\n            print(c.CUSTOMER, c.WORD_LIST)\n            print(c.MATCHING_CUSTOMERS)\n            print(c.PERCENT_DIFFERENCE)\n            print(c.DAYS_DIFFERENCE)\n            print(\"\u003d\u003d\u003d\u003d\u003d\")\n            print()\n\n    else:\n\n        # could not find a match, remove it from future processing\n        _no_match_customer.append(c.CUSTOMER)\n        _processed_customers.append(c.CUSTOMER)\n\n    if to_save_counter\u003e\u003dsave_every_n:\n\n        _matching_process_log_time.append(str(pd.Timestamp.now()))\n        _matching_process_log_event.append(\"writing datasets to snowflake\")\n        do_save_log(_matching_process_log_time, _matching_process_log_event)\n\n        do_save_direct_matches(_direct_customer, _direct_match, _direct_draw_up_date)\n        do_save_multiple_matches(_multiple_customer, _multiple_matches, _multiple_drop_dates)\n\n        _matching_process_log_time.append(str(pd.Timestamp.now()))\n        _matching_process_log_event.append(\"saved \" + str(to_save_counter) + \" records to snowflake.\")\n        do_save_log(_matching_process_log_time, _matching_process_log_event)\n\n        to_save_counter \u003d 0\n        \n    t1 \u003d time.time()\n    \n    avg_duration \u003d (((t1-t0)/idx)/60.0)\n    \n    if idx % print_every_n \u003d\u003d 0:\n        idx_remaining \u003d len(_customers)-idx\n        print(\"processing\", idx, \"current record:\", c.CUSTOMER, \",\", idx_remaining, \"remaining\")\n        print(round(avg_duration,2), \"avg mins per iteration\",  round((avg_duration*idx_remaining)/60,2), \"estimated hrs remaining\")\n        print(len(_direct_customer), \"direct match records\", len(_multiple_customer), \"multiple match records\", len(_no_match_customer), \"no match records\")\n        print()\n        \n_matching_process_log_time.append(str(pd.Timestamp.now()))\n_matching_process_log_event.append(\"writing datasets to snowflake\")\ndo_save_log(_matching_process_log_time, _matching_process_log_event)\n\ndo_save_direct_matches(_direct_customer, _direct_match, _direct_draw_up_date)\ndo_save_multiple_matches(_multiple_customer, _multiple_matches, _multiple_drop_dates)\n\n_matching_process_log_time.append(str(pd.Timestamp.now()))\n_matching_process_log_event.append(\"saved \" + str(to_save_counter) + \" records to snowflake.\")\ndo_save_log(_matching_process_log_time, _matching_process_log_event)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute recipe outputs\n# TODO: Write here your actual code that computes the outputs\n# NB: DSS supports several kinds of APIs for reading and writing data. Please see doc.\n\n#MATCHES_1_TO_N_STAGING_V_df \u003d ... # Compute a Pandas dataframe to write into MATCHES_1_TO_N_STAGING_V\n#MATCHES_1_TO_1_STAGING_V_df \u003d ... # Compute a Pandas dataframe to write into MATCHES_1_TO_1_STAGING_V\n\n\n# Write recipe outputs\n#MATCHES_1_TO_N_STAGING_V \u003d dataiku.Dataset(\"MATCHES_1_TO_N_STAGING_V\")\n#MATCHES_1_TO_N_STAGING_V.write_with_schema(MATCHES_1_TO_N_STAGING_V_df)\n#MATCHES_1_TO_1_STAGING_V \u003d dataiku.Dataset(\"MATCHES_1_TO_1_STAGING_V\")\n#MATCHES_1_TO_1_STAGING_V.write_with_schema(MATCHES_1_TO_1_STAGING_V_df)"
      ],
      "outputs": []
    }
  ]
}