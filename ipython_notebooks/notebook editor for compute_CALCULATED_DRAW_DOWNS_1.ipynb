{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-env_clc",
      "display_name": "Python (env env_clc)",
      "language": "python"
    },
    "associatedRecipe": "compute_CALCULATED_DRAW_DOWNS",
    "dkuGit": {
      "lastInteraction": 0
    },
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "Daniel.Vandermeer"
      },
      "lastModifiedOn": 1667062120007
    },
    "creator": "Daniel.Vandermeer",
    "createdOn": 1667062120007,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {},
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "Daniel.Vandermeer"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 67,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\nfrom dateutil.relativedelta import relativedelta\nfrom helper import *\nimport time\n\n# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\nimport pickle\nfrom dateutil.relativedelta import relativedelta\nimport gc\nfrom re import finditer\n\n## Find DD DU\nfrom helper import preprocess_data\nfrom patterns import find_drawdowns, find_drawups\n\n## MATCHING\nimport name_matching\nfrom name_matching import name_match\nimport transaction_matching\nfrom transaction_matching import transaction_match\n\n## CONSOLIDATION\nfrom consolidation import combine_matches, consolidate_matches, find_attritions, find_new_accounts, get_attrition_status, get_new_account_status"
      ],
      "outputs": []
    },
    {
      "execution_count": 68,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "start_date \u003d dataiku.get_custom_variables()[\u0027start_date\u0027]\nend_date \u003d dataiku.get_custom_variables()[\u0027end_date\u0027]\n\nconsistency \u003d int(dataiku.get_custom_variables()[\u0027consistency\u0027])\ndrawdown_period_average \u003d int(dataiku.get_custom_variables()[\u0027drawdown_period_average\u0027])\ndrawdown \u003d int(dataiku.get_custom_variables()[\u0027drawdown\u0027])\ndrawdown_fwd_check \u003d int(dataiku.get_custom_variables()[\u0027drawdown_fwd_check\u0027])\ndrawdown_lookback_period \u003d int(dataiku.get_custom_variables()[\u0027drawdown_lookback_period\u0027])\ndrawup_lookfwd_period \u003d int(dataiku.get_custom_variables()[\u0027drawup_lookfwd_period\u0027])\nstatistics_period \u003d int(dataiku.get_custom_variables()[\u0027statistics_period\u0027])\ninactive_period \u003d int(dataiku.get_custom_variables()[\u0027inactive_period\u0027])\n\n## MATCHING VARIABLES\nmonth_diff_h \u003d int(dataiku.get_custom_variables()[\u0027month_diff_h\u0027])\nmonth_diff_l \u003d int(dataiku.get_custom_variables()[\u0027month_diff_l\u0027])\nsd_mul \u003d int(dataiku.get_custom_variables()[\u0027sd_mul\u0027])\nmax_city_distance \u003d int(dataiku.get_custom_variables()[\u0027max_city_distance\u0027])\nthreshold_score_step1 \u003d int(dataiku.get_custom_variables()[\u0027threshold_score_step1\u0027])\nthreshold_score_step2 \u003d int(dataiku.get_custom_variables()[\u0027threshold_score_step2\u0027])\n\n## RUN TYPE\nrun \u003d dataiku.get_custom_variables()[\u0027run_type\u0027]\n\nprint(\"start_date\", start_date)\nprint(\"end_date\", end_date)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "start_date 2019-01-01\nend_date 2022-10-01\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 69,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read recipe inputs\nNAFCUSTOMER_ACTIVE_CARDS_FULL \u003d dataiku.Dataset(\"NAFCUSTOMER_ACTIVE_CARDS_FULL\")\nNAFCUSTOMER_ACTIVE_CARDS_FULL_df \u003d NAFCUSTOMER_ACTIVE_CARDS_FULL.get_dataframe()\nprint(len(NAFCUSTOMER_ACTIVE_CARDS_FULL_df))\nNAFCUSTOMER_ACTIVE_CARDS_FULL_df.head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "16564685\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 69,
          "data": {
            "text/plain": "                              CUSTOMER  REVENUE_YEAR  REVENUE_MONTH  ACTIVE_CARD_COUNT\n0                   547121 ONT LTD O/A          2019              1               16.0\n1        PARTNERS IN MISSION FOOD BANK          2019              1                1.0\n2       BEKINS A 1 MOVERS INC. (CHICAG          2019              1               10.0\n3                B LEVEL SOUTHEAST INC          2019              1                2.0\n4  COLDSMITHS CONSTRUCTION COMPANY INC          2019              1               11.0",
            "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style\u003d\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003eCUSTOMER\u003c/th\u003e\n      \u003cth\u003eREVENUE_YEAR\u003c/th\u003e\n      \u003cth\u003eREVENUE_MONTH\u003c/th\u003e\n      \u003cth\u003eACTIVE_CARD_COUNT\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003e547121 ONT LTD O/A\u003c/td\u003e\n      \u003ctd\u003e2019\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e16.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003ePARTNERS IN MISSION FOOD BANK\u003c/td\u003e\n      \u003ctd\u003e2019\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e1.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003eBEKINS A 1 MOVERS INC. (CHICAG\u003c/td\u003e\n      \u003ctd\u003e2019\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e10.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003eB LEVEL SOUTHEAST INC\u003c/td\u003e\n      \u003ctd\u003e2019\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e2.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003eCOLDSMITHS CONSTRUCTION COMPANY INC\u003c/td\u003e\n      \u003ctd\u003e2019\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e11.0\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          },
          "metadata": {}
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def date_tz_naive(pd_s):\n    return pd.to_datetime(pd_s).apply(lambda x:x.tz_localize(None))"
      ],
      "outputs": []
    },
    {
      "execution_count": 25,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "customer_list_full \u003d NAFCUSTOMER_ACTIVE_CARDS_FULL_df.CUSTOMER.unique()\nprint(len(customer_list_full))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "624069\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 74,
      "cell_type": "code",
      "metadata": {
        "scrolled": false
      },
      "source": [
        "page_size \u003d 50000\nidx \u003d 0\ncurrent_page \u003d 0\nmax_pages \u003d 0\n\ndrop_df \u003d pd.DataFrame()\nt0 \u003d time.time()\n\ntotal_pages \u003d len(customer_list_full)/page_size\n\nwhile idx\u003clen(customer_list_full):\n    \n    current_page+\u003d1\n    print(\"page\", current_page)\n        \n    to_range \u003d idx+page_size\n    if to_range\u003elen(customer_list_full):\n        to_range \u003d len(customer_list_full)-1\n    \n    current_set \u003d customer_list_full[idx:to_range]\n\n    #\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n    \n    df_v \u003d NAFCUSTOMER_ACTIVE_CARDS_FULL_df[NAFCUSTOMER_ACTIVE_CARDS_FULL_df.CUSTOMER.isin(current_set)]\n    print(\"processing\", len(df_v.CUSTOMER.unique()), \"customers\")\n    print(len(df_v), \"data frame records\")\n    \n    df_v[\u0027REVENUE_DATE\u0027] \u003d df_v.REVENUE_MONTH.astype(str) + \"/01/\" + df_v.REVENUE_YEAR.astype(str)\n    df_v[\u0027REVENUE_DATE\u0027] \u003d date_tz_naive(df_v[\u0027REVENUE_DATE\u0027])\n\n    df_v \u003d df_v[[\u0027CUSTOMER\u0027,\u0027REVENUE_DATE\u0027, \u0027ACTIVE_CARD_COUNT\u0027]]\n\n    df_v_max \u003d df_v[[\u0027CUSTOMER\u0027,\u0027ACTIVE_CARD_COUNT\u0027]]\n    df_max \u003d df_v_max.groupby(by\u003d[\"CUSTOMER\"]).max().reset_index()\n    df_max.columns \u003d [\u0027CUSTOMER\u0027, \u0027ACTIVE_CARD_MAX\u0027]\n\n    df_v.dropna(subset\u003d[\u0027CUSTOMER\u0027], inplace\u003dTrue)\n\n    df_v[\u0027REVENUE_DATE\u0027] \u003d pd.to_datetime(df_v[\u0027REVENUE_DATE\u0027])\n    df_v.sort_values([\u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n\n    seen_accounts \u003d df_v[df_v[\u0027ACTIVE_CARD_COUNT\u0027] \u003e 0].groupby([\u0027CUSTOMER\u0027], as_index\u003dFalse)[[\u0027REVENUE_DATE\u0027]].first()\n    seen_accounts[\u0027FIRST_DATE\u0027] \u003d seen_accounts[\u0027REVENUE_DATE\u0027] - pd.DateOffset(months\u003d1)\n\n    df \u003d df_v\n    period_end_date \u003d end_date\n    match_type \u003d \u0027program_flip\u0027\n    period_start_date\u003dNone\n    split\u003dNone\n\n    drawdown \u003d (100 - drawdown)/100\n    drawdown_fwd_check /\u003d 100\n\n    inactive_date_start \u003d pd.to_datetime(period_end_date) + relativedelta(months\u003d-inactive_period)\n\n    df \u003d df[df[\u0027REVENUE_DATE\u0027] \u003c\u003d period_end_date].copy()\n\n    if period_start_date:\n        period_start_date \u003d pd.to_datetime(period_start_date)\n        df \u003d df[df[\u0027REVENUE_DATE\u0027] \u003e\u003d period_start_date].copy()\n\n    all_customer_names \u003d list(df[\u0027CUSTOMER\u0027].unique())\n    \n    dd_find \u003d df[df[\u0027CUSTOMER\u0027].isin(all_customer_names)].copy()\n\n    consistent_customers_dd \u003d find_consistent_cust(dd_find, consecutive\u003dconsistency)\n    if len(consistent_customers_dd) \u003d\u003d 0:\n        continue\n\n    dd_find \u003d dd_find[dd_find[\u0027CUSTOMER\u0027].isin(consistent_customers_dd)].copy()\n    dd_find \u003d add_padding_func(dd_find, padding\u003dstatistics_period, last_date\u003dperiod_end_date)\n    dd_find \u003d find_average_func(dd_find, n\u003dstatistics_period)\n\n    dd_find[\u0027DD_INDICATOR\u0027] \u003d np.where(((drawdown*(dd_find[\u0027LAST_N_MONTHS_AVG\u0027].round(3)) \u003e\n                                     dd_find[\u0027ACTIVE_CARD_COUNT\u0027].round(3)) \u0026\n                                    (dd_find[\u0027NEXT_N_MONTHS_AVG\u0027].round(3) \u003c\n                                     drawdown_fwd_check*dd_find[\u0027LAST_N_MONTHS_AVG\u0027].round(3))),\n                                   True, False)\n\n    ## Find the first drawdown and also the list of customers\n    pflip_dd \u003d dd_find[dd_find[\u0027DD_INDICATOR\u0027] \u003d\u003d True].copy()\n    pflip_dd.drop_duplicates(\u0027CUSTOMER\u0027, inplace\u003dTrue)\n    first_drop_idx \u003d pflip_dd.index\n    pflip_dd_customers \u003d list(dd_find[\u0027CUSTOMER\u0027].unique())\n    first_drop \u003d dd_find.iloc[first_drop_idx]\n\n    ## Identify the lookback period\n    first_drop \u003d first_drop[[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027]].copy()\n    first_drop \u003d first_drop[first_drop[\u0027REVENUE_DATE\u0027] \u003c\u003d inactive_date_start].copy()\n    first_drop[\u0027START_DATE\u0027]  \u003d first_drop[\u0027REVENUE_DATE\u0027] - pd.DateOffset(months\u003ddrawdown_lookback_period)\n    first_drop.rename(columns \u003d {\u0027REVENUE_DATE\u0027:\u0027DD_DATE\u0027}, inplace\u003dTrue)\n    dd_find_df \u003d dd_find[dd_find[\u0027CUSTOMER\u0027].isin(pflip_dd_customers)]\n    dd_find_df \u003d dd_find_df.merge(first_drop, on\u003d[\u0027CUSTOMER\u0027])\n    dd_find_df \u003d dd_find_df[dd_find_df[\u0027REVENUE_DATE\u0027].between(dd_find_df[\u0027START_DATE\u0027],dd_find_df[\u0027DD_DATE\u0027])].copy()\n\n    ## Compute the sharpest fall from the lookback period\n    dd_find_df.sort_values([\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n    dd_find_df[\u0027DROP\u0027] \u003d dd_find_df.groupby([\u0027CUSTOMER\u0027])[\u0027ACTIVE_CARD_COUNT\u0027].diff(-1)\n\n    ## Find the corresponding period and remove duplicates in case of a similar values\n    drop_idx \u003d dd_find_df.groupby([\u0027CUSTOMER\u0027])[\u0027DROP\u0027].transform(max) \u003d\u003d dd_find_df[\u0027DROP\u0027]\n    drop_month_df \u003d dd_find_df[drop_idx].copy()\n    drop_month_df.drop_duplicates([\u0027CUSTOMER\u0027], inplace\u003dTrue)\n\n    ## remove the first record\n    dd_find \u003d dd_find.groupby(\u0027CUSTOMER\u0027).apply(lambda group: group.iloc[1:, 1:]).reset_index()\n    dd_find.drop(\u0027level_1\u0027, axis\u003d1, inplace\u003dTrue)\n\n    ## Find the time periods for calculating statistics (mean and standard deviation)\n    drop_month_df.rename(columns \u003d {\u0027REVENUE_DATE\u0027:\u0027DROP_DATE\u0027}, inplace\u003dTrue)\n    dd_find \u003d dd_find.merge(drop_month_df[[\u0027CUSTOMER\u0027, \u0027DROP_DATE\u0027]], on\u003d\u0027CUSTOMER\u0027)\n    dd_find[\u0027END_DATE\u0027] \u003d dd_find[\u0027DROP_DATE\u0027] - pd.DateOffset(months\u003d3)\n    dd_find[\u0027START_DATE\u0027] \u003d dd_find[\u0027END_DATE\u0027] - pd.DateOffset(months\u003dstatistics_period-1)\n    pflip_12_data \u003d dd_find[dd_find[\u0027REVENUE_DATE\u0027].between(dd_find[\u0027START_DATE\u0027], dd_find[\u0027END_DATE\u0027])].copy()\n\n    ## Calculate Mean and Standard Deviation\n    dd_stat \u003d pflip_12_data.groupby([\u0027CUSTOMER\u0027], as_index\u003dFalse).agg({\u0027ACTIVE_CARD_COUNT\u0027:[\u0027mean\u0027,\u0027std\u0027]})\n    dd_stat.columns \u003d [\u0027CUSTOMER_DD\u0027, \u0027MEAN_DD\u0027,\u0027STD_DD\u0027]\n    drop_month_df \u003d drop_month_df.merge(dd_stat,\n                                        left_on\u003d\u0027CUSTOMER\u0027,\n                                        right_on\u003d\u0027CUSTOMER_DD\u0027,\n                                        how\u003d\u0027left\u0027)\n\n    drop_month_df \u003d pd.merge(drop_month_df, df_max, on\u003d\u0027CUSTOMER\u0027, how\u003d\u0027left\u0027)\n    drop_month_df \u003d drop_month_df[[\u0027CUSTOMER\u0027,\u0027DROP_DATE\u0027,\u0027ACTIVE_CARD_MAX\u0027]]\n    \n    print(len(drop_month_df), \"new drop records\")\n    drop_df \u003d pd.concat([drop_df, drop_month_df], ignore_index\u003dTrue)\n    \n    print(len(drop_df), \"total drop records\")\n    print(\"saving to snowflake...\")\n    CALCULATED_DRAW_DOWNS_df \u003d drop_df\n    CALCULATED_DRAW_DOWNS \u003d dataiku.Dataset(\"CALCULATED_DRAW_DOWNS\")\n    CALCULATED_DRAW_DOWNS.write_with_schema(CALCULATED_DRAW_DOWNS_df)\n    \n    pages_remaining \u003d total_pages-current_page\n    \n    t1 \u003d time.time()\n    avg_duration \u003d (((t1-t0)/current_page)/60.0)\n    print(round(avg_duration,2), \"avg mins per iteration\")\n    print(round(pages_remaining,2), \"pages remaining\")\n    print(round(avg_duration*pages_remaining,2), \"estimated minutes remaining\")\n    print()\n    \n    #\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n    \n    idx+\u003dpage_size\n    \n    if max_pages\u003e0:\n        if current_page\u003e\u003dmax_pages:\n            break;\n            \nprint(\"done\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "page 1\nprocessing 994 customers\n36278 data frame records\n265 new drop records\n265 total drop records\nsaving to snowflake...\n265 rows successfully written (oUdKpTDsCH)\n0.17 avg mins per iteration\n623.07 pages remaining\n103.37 estimated minutes remaining\n\npage 2\nprocessing 995 customers\n36335 data frame records\n262 new drop records\n527 total drop records\nsaving to snowflake...\n527 rows successfully written (yKfRwCLqDw)\n0.14 avg mins per iteration\n622.07 pages remaining\n89.51 estimated minutes remaining\n\ndone\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def find_consistent_cust(df, consecutive\u003d3):\n    \u0027\u0027\u0027returns a list of customers who are consistent for 3 (default value) months\u0027\u0027\u0027\n\n    ## Needs only these columns [\u0027customer_account_name\u0027, \u0027revenue_month\u0027, \u0027purchase_gallons_qty\u0027]\n\n    df \u003d df[[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027, \u0027ACTIVE_CARD_COUNT\u0027]].copy()\n    df.sort_values(by\u003d[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n\n    z \u003d (df.groupby([\u0027CUSTOMER\u0027])[\u0027REVENUE_DATE\u0027].diff(1)/np.timedelta64(1, \u0027M\u0027))\n    z \u003d z.round(0)\n    z \u003d (z \u003d\u003d 1).astype(\u0027int\u0027)\n    df[\u0027CUST_CONS\u0027] \u003d (z * (z.groupby((z !\u003d z.shift()).cumsum()).cumcount() + 2))\n    cust_cons \u003d df.groupby(\u0027CUSTOMER\u0027)[\u0027CUST_CONS\u0027].max()\n\n    return list(cust_cons[cust_cons\u003e\u003dconsecutive].index)\n\ndef add_padding_func(df, padding\u003d12, last_date\u003dNone):\n    \u0027\u0027\u0027\n    Fills all the zeros in between for intermittent data and also fills the trailing data with\n    12 zeros or till the last date whichever is earlier\n    \u0027\u0027\u0027\n\n    cols \u003d [\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027]\n\n    common_cols \u003d set(df.columns).intersection(set(cols))\n\n    profile \u003d df[common_cols].drop_duplicates()\n\n    vol \u003d df[[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027, \u0027ACTIVE_CARD_COUNT\u0027]].copy()\n    vol \u003d vol.groupby([\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027])[[\u0027ACTIVE_CARD_COUNT\u0027]].sum().reset_index()\n    vol.reset_index(drop\u003dTrue, inplace\u003dTrue)\n\n    vol.sort_values([\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n    vol.reset_index(drop\u003dTrue, inplace\u003dTrue)\n\n    last_rev_date \u003d vol.groupby([\u0027CUSTOMER\u0027])[[\u0027REVENUE_DATE\u0027]].last()\n    last_rev_date \u003d last_rev_date[last_rev_date[\u0027REVENUE_DATE\u0027] \u003c pd.to_datetime(last_date)]\n    last_rev_date[\u0027REVENUE_DATE\u0027] \u003d last_rev_date[\u0027REVENUE_DATE\u0027] + pd.DateOffset(months\u003dpadding)\n    last_rev_date[\u0027LAST_DATE\u0027] \u003d pd.to_datetime(last_date)\n    last_rev_date[\u0027REVENUE_DATE\u0027] \u003d last_rev_date[[\u0027REVENUE_DATE\u0027,\u0027LAST_DATE\u0027]].min(axis\u003d1)\n    last_rev_date.drop([\u0027LAST_DATE\u0027], axis\u003d1, inplace\u003dTrue)\n    last_rev_date.reset_index(inplace\u003dTrue)\n    vol \u003d pd.concat([vol, last_rev_date], ignore_index\u003dTrue)\n    vol.fillna(0, inplace\u003dTrue)\n    vol \u003d (vol.set_index(\u0027REVENUE_DATE\u0027).groupby(\u0027CUSTOMER\u0027).resample(\u0027MS\u0027).asfreq()\n                  .drop([\u0027CUSTOMER\u0027], 1).reset_index())\n    vol.fillna(0, inplace\u003dTrue)\n    df \u003d vol.merge(profile, how\u003d\u0027left\u0027, on \u003d [\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027])\n    df.fillna(method\u003d\u0027ffill\u0027, inplace\u003dTrue)\n\n    return df\n\ndef find_average_func(dd_find, n\u003d12):\n\n    dd_find.sort_values([\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n    dd_find2 \u003d dd_find.sort_values([\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027], ascending\u003d[True, False]).reset_index(drop\u003dTrue)\n\n    dd_find.reset_index(drop\u003dTrue, inplace\u003dTrue)\n    dd_find[\u0027LAST_N_MONTHS_AVG\u0027] \u003d dd_find.groupby([\u0027CUSTOMER\u0027])[\u0027ACTIVE_CARD_COUNT\u0027]\\\n                                        .rolling(n, min_periods\u003d1).mean().reset_index(drop\u003dTrue)\n    dd_find2[\u0027NEXT_N_MONTHS_AVG\u0027] \u003d dd_find2.groupby([\u0027CUSTOMER\u0027])[\u0027ACTIVE_CARD_COUNT\u0027]\\\n                                        .rolling(n, min_periods\u003d1).mean().reset_index(drop\u003dTrue)\n\n    dd_find[\u0027LAST_N_MONTHS_AVG\u0027] \u003d dd_find.groupby(\u0027CUSTOMER\u0027)[\u0027LAST_N_MONTHS_AVG\u0027].shift(1)\n    dd_find2[\u0027NEXT_N_MONTHS_AVG\u0027] \u003d dd_find2.groupby(\u0027CUSTOMER\u0027)[\u0027NEXT_N_MONTHS_AVG\u0027].shift(1)\n\n    dd_find \u003d dd_find.merge(dd_find2[[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027, \u0027NEXT_N_MONTHS_AVG\u0027]],\n                on\u003d[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027])\n\n    return dd_find"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}