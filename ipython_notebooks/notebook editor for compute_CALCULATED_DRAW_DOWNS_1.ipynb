{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-env_clc-cpu-m-cpu-3-mem-4gb",
      "display_name": "Python in cpu-m-cpu-3-mem-4Gb (env env_clc)",
      "language": "python"
    },
    "associatedRecipe": "compute_CALCULATED_DRAW_DOWNS",
    "dkuGit": {
      "lastInteraction": 0
    },
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "Daniel.Vandermeer"
      },
      "lastModifiedOn": 1667062120007
    },
    "creator": "Daniel.Vandermeer",
    "createdOn": 1667062120007,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {},
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "Daniel.Vandermeer"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\nfrom dateutil.relativedelta import relativedelta\nfrom helper import *\nimport time\n\n# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\nimport pickle\nfrom dateutil.relativedelta import relativedelta\nimport gc\nfrom re import finditer\n\n## Find DD DU\nfrom helper import preprocess_data\nfrom patterns import find_drawdowns, find_drawups\n\n## MATCHING\nimport name_matching\nfrom name_matching import name_match\nimport transaction_matching\nfrom transaction_matching import transaction_match\n\n## CONSOLIDATION\nfrom consolidation import combine_matches, consolidate_matches, find_attritions, find_new_accounts, get_attrition_status, get_new_account_status"
      ],
      "outputs": []
    },
    {
      "execution_count": 2,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "start_date \u003d dataiku.get_custom_variables()[\u0027start_date\u0027]\nend_date \u003d dataiku.get_custom_variables()[\u0027end_date\u0027]\n\nconsistency \u003d int(dataiku.get_custom_variables()[\u0027consistency\u0027])\ndrawdown_period_average \u003d int(dataiku.get_custom_variables()[\u0027drawdown_period_average\u0027])\ndrawdown \u003d int(dataiku.get_custom_variables()[\u0027drawdown\u0027])\ndrawdown_fwd_check \u003d int(dataiku.get_custom_variables()[\u0027drawdown_fwd_check\u0027])\ndrawdown_lookback_period \u003d int(dataiku.get_custom_variables()[\u0027drawdown_lookback_period\u0027])\ndrawup_lookfwd_period \u003d int(dataiku.get_custom_variables()[\u0027drawup_lookfwd_period\u0027])\nstatistics_period \u003d int(dataiku.get_custom_variables()[\u0027statistics_period\u0027])\ninactive_period \u003d int(dataiku.get_custom_variables()[\u0027inactive_period\u0027])\n\n## MATCHING VARIABLES\nmonth_diff_h \u003d int(dataiku.get_custom_variables()[\u0027month_diff_h\u0027])\nmonth_diff_l \u003d int(dataiku.get_custom_variables()[\u0027month_diff_l\u0027])\nsd_mul \u003d int(dataiku.get_custom_variables()[\u0027sd_mul\u0027])\nmax_city_distance \u003d int(dataiku.get_custom_variables()[\u0027max_city_distance\u0027])\nthreshold_score_step1 \u003d int(dataiku.get_custom_variables()[\u0027threshold_score_step1\u0027])\nthreshold_score_step2 \u003d int(dataiku.get_custom_variables()[\u0027threshold_score_step2\u0027])\n\n## RUN TYPE\nrun \u003d dataiku.get_custom_variables()[\u0027run_type\u0027]\n\nprint(\"start_date\", start_date)\nprint(\"end_date\", end_date)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "start_date 2019-01-01\nend_date 2022-11-01\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 3,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read recipe inputs\nNAFCUSTOMER_ACTIVE_CARDS_FULL \u003d dataiku.Dataset(\"NAFCUSTOMER_ACTIVE_CARDS_FULL\")\nNAFCUSTOMER_ACTIVE_CARDS_FULL_df \u003d NAFCUSTOMER_ACTIVE_CARDS_FULL.get_dataframe()\nprint(len(NAFCUSTOMER_ACTIVE_CARDS_FULL_df))\nNAFCUSTOMER_ACTIVE_CARDS_FULL_df.head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "16867349\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "                   CUSTOMER  REVENUE_YEAR  REVENUE_MONTH  ACTIVE_CARD_COUNT\n0            I S O RX TEXAS          2019              1                8.0\n1  BRENTWOOD LANDSCAPES INC          2019              1                7.0\n2  BROWN DOWNHOLE TOOLS LLC          2019              1                4.0\n3           ADDISON LEASING          2019              1               13.0\n4     HARDIN BAPTIST CHURCH          2019              1                1.0",
            "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style\u003d\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003eCUSTOMER\u003c/th\u003e\n      \u003cth\u003eREVENUE_YEAR\u003c/th\u003e\n      \u003cth\u003eREVENUE_MONTH\u003c/th\u003e\n      \u003cth\u003eACTIVE_CARD_COUNT\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003eI S O RX TEXAS\u003c/td\u003e\n      \u003ctd\u003e2019\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e8.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003eBRENTWOOD LANDSCAPES INC\u003c/td\u003e\n      \u003ctd\u003e2019\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e7.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003eBROWN DOWNHOLE TOOLS LLC\u003c/td\u003e\n      \u003ctd\u003e2019\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e4.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003eADDISON LEASING\u003c/td\u003e\n      \u003ctd\u003e2019\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e13.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003eHARDIN BAPTIST CHURCH\u003c/td\u003e\n      \u003ctd\u003e2019\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e1.0\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          },
          "metadata": {}
        }
      ]
    },
    {
      "execution_count": 4,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def date_tz_naive(pd_s):\n    return pd.to_datetime(pd_s).apply(lambda x:x.tz_localize(None))"
      ],
      "outputs": []
    },
    {
      "execution_count": 5,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "customer_list_full \u003d NAFCUSTOMER_ACTIVE_CARDS_FULL_df.CUSTOMER.unique()\nprint(len(customer_list_full))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "634803\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 6,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def find_consistent_cust(df, consecutive\u003d3):\n    \u0027\u0027\u0027returns a list of customers who are consistent for 3 (default value) months\u0027\u0027\u0027\n\n    ## Needs only these columns [\u0027customer_account_name\u0027, \u0027revenue_month\u0027, \u0027purchase_gallons_qty\u0027]\n\n    df \u003d df[[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027, \u0027ACTIVE_CARD_COUNT\u0027]].copy()\n    df.sort_values(by\u003d[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n\n    z \u003d (df.groupby([\u0027CUSTOMER\u0027])[\u0027REVENUE_DATE\u0027].diff(1)/np.timedelta64(1, \u0027M\u0027))\n    z \u003d z.round(0)\n    z \u003d (z \u003d\u003d 1).astype(\u0027int\u0027)\n    df[\u0027CUST_CONS\u0027] \u003d (z * (z.groupby((z !\u003d z.shift()).cumsum()).cumcount() + 2))\n    cust_cons \u003d df.groupby(\u0027CUSTOMER\u0027)[\u0027CUST_CONS\u0027].max()\n\n    return list(cust_cons[cust_cons\u003e\u003dconsecutive].index)\n\ndef add_padding_func(df, padding\u003d12, last_date\u003dNone):\n    \u0027\u0027\u0027\n    Fills all the zeros in between for intermittent data and also fills the trailing data with\n    12 zeros or till the last date whichever is earlier\n    \u0027\u0027\u0027\n\n    cols \u003d [\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027]\n\n    common_cols \u003d set(df.columns).intersection(set(cols))\n\n    profile \u003d df[common_cols].drop_duplicates()\n\n    vol \u003d df[[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027, \u0027ACTIVE_CARD_COUNT\u0027]].copy()\n    vol \u003d vol.groupby([\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027])[[\u0027ACTIVE_CARD_COUNT\u0027]].sum().reset_index()\n    vol.reset_index(drop\u003dTrue, inplace\u003dTrue)\n\n    vol.sort_values([\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n    vol.reset_index(drop\u003dTrue, inplace\u003dTrue)\n\n    last_rev_date \u003d vol.groupby([\u0027CUSTOMER\u0027])[[\u0027REVENUE_DATE\u0027]].last()\n    last_rev_date \u003d last_rev_date[last_rev_date[\u0027REVENUE_DATE\u0027] \u003c pd.to_datetime(last_date)]\n    last_rev_date[\u0027REVENUE_DATE\u0027] \u003d last_rev_date[\u0027REVENUE_DATE\u0027] + pd.DateOffset(months\u003dpadding)\n    last_rev_date[\u0027LAST_DATE\u0027] \u003d pd.to_datetime(last_date)\n    last_rev_date[\u0027REVENUE_DATE\u0027] \u003d last_rev_date[[\u0027REVENUE_DATE\u0027,\u0027LAST_DATE\u0027]].min(axis\u003d1)\n    last_rev_date.drop([\u0027LAST_DATE\u0027], axis\u003d1, inplace\u003dTrue)\n    last_rev_date.reset_index(inplace\u003dTrue)\n    vol \u003d pd.concat([vol, last_rev_date], ignore_index\u003dTrue)\n    vol.fillna(0, inplace\u003dTrue)\n    vol \u003d (vol.set_index(\u0027REVENUE_DATE\u0027).groupby(\u0027CUSTOMER\u0027).resample(\u0027MS\u0027).asfreq()\n                  .drop([\u0027CUSTOMER\u0027], 1).reset_index())\n    vol.fillna(0, inplace\u003dTrue)\n    df \u003d vol.merge(profile, how\u003d\u0027left\u0027, on \u003d [\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027])\n    df.fillna(method\u003d\u0027ffill\u0027, inplace\u003dTrue)\n\n    return df\n\ndef find_average_func(dd_find, n\u003d12):\n\n    dd_find.sort_values([\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n    dd_find2 \u003d dd_find.sort_values([\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027], ascending\u003d[True, False]).reset_index(drop\u003dTrue)\n\n    dd_find.reset_index(drop\u003dTrue, inplace\u003dTrue)\n    dd_find[\u0027LAST_N_MONTHS_AVG\u0027] \u003d dd_find.groupby([\u0027CUSTOMER\u0027])[\u0027ACTIVE_CARD_COUNT\u0027]\\\n                                        .rolling(n, min_periods\u003d1).mean().reset_index(drop\u003dTrue)\n    dd_find2[\u0027NEXT_N_MONTHS_AVG\u0027] \u003d dd_find2.groupby([\u0027CUSTOMER\u0027])[\u0027ACTIVE_CARD_COUNT\u0027]\\\n                                        .rolling(n, min_periods\u003d1).mean().reset_index(drop\u003dTrue)\n\n    dd_find[\u0027LAST_N_MONTHS_AVG\u0027] \u003d dd_find.groupby(\u0027CUSTOMER\u0027)[\u0027LAST_N_MONTHS_AVG\u0027].shift(1)\n    dd_find2[\u0027NEXT_N_MONTHS_AVG\u0027] \u003d dd_find2.groupby(\u0027CUSTOMER\u0027)[\u0027NEXT_N_MONTHS_AVG\u0027].shift(1)\n\n    dd_find \u003d dd_find.merge(dd_find2[[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027, \u0027NEXT_N_MONTHS_AVG\u0027]],\n                on\u003d[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027])\n\n    return dd_find"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "page_size \u003d 50000\nidx \u003d 0\ncurrent_page \u003d 0\nmax_pages \u003d 0\n\ndrop_df \u003d pd.DataFrame()\nt0 \u003d time.time()\n\ntotal_pages \u003d len(customer_list_full)/page_size\n\nwhile idx\u003clen(customer_list_full):\n\n    current_page+\u003d1\n    print(\"page\", current_page)\n\n    to_range \u003d idx+page_size\n    if to_range\u003elen(customer_list_full):\n        to_range \u003d len(customer_list_full)-1\n\n    current_set \u003d customer_list_full[idx:to_range]\n\n    #\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\n    df_v \u003d NAFCUSTOMER_ACTIVE_CARDS_FULL_df[NAFCUSTOMER_ACTIVE_CARDS_FULL_df.CUSTOMER.isin(current_set)]\n    print(\"processing\", len(df_v.CUSTOMER.unique()), \"customers\")\n    print(len(df_v), \"data frame records\")\n\n    df_v[\u0027REVENUE_DATE\u0027] \u003d df_v.REVENUE_MONTH.astype(str) + \"/01/\" + df_v.REVENUE_YEAR.astype(str)\n    df_v[\u0027REVENUE_DATE\u0027] \u003d date_tz_naive(df_v[\u0027REVENUE_DATE\u0027])\n\n    df_v \u003d df_v[[\u0027CUSTOMER\u0027,\u0027REVENUE_DATE\u0027, \u0027ACTIVE_CARD_COUNT\u0027]]\n\n    df_v_max \u003d df_v[[\u0027CUSTOMER\u0027,\u0027ACTIVE_CARD_COUNT\u0027]]\n    df_max \u003d df_v_max.groupby(by\u003d[\"CUSTOMER\"]).max().reset_index()\n    df_max.columns \u003d [\u0027CUSTOMER\u0027, \u0027ACTIVE_CARD_MAX\u0027]\n\n    df_v.dropna(subset\u003d[\u0027CUSTOMER\u0027], inplace\u003dTrue)\n\n    df_v[\u0027REVENUE_DATE\u0027] \u003d pd.to_datetime(df_v[\u0027REVENUE_DATE\u0027])\n    df_v.sort_values([\u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n\n    seen_accounts \u003d df_v[df_v[\u0027ACTIVE_CARD_COUNT\u0027] \u003e 0].groupby([\u0027CUSTOMER\u0027], as_index\u003dFalse)[[\u0027REVENUE_DATE\u0027]].first()\n    seen_accounts[\u0027FIRST_DATE\u0027] \u003d seen_accounts[\u0027REVENUE_DATE\u0027] - pd.DateOffset(months\u003d1)\n\n    df \u003d df_v\n    period_end_date \u003d end_date\n    match_type \u003d \u0027program_flip\u0027\n    period_start_date\u003dNone\n    split\u003dNone\n\n    drawdown \u003d (100 - drawdown)/100\n    drawdown_fwd_check /\u003d 100\n\n    inactive_date_start \u003d pd.to_datetime(period_end_date) + relativedelta(months\u003d-inactive_period)\n\n    df \u003d df[df[\u0027REVENUE_DATE\u0027] \u003c\u003d period_end_date].copy()\n\n    if period_start_date:\n        period_start_date \u003d pd.to_datetime(period_start_date)\n        df \u003d df[df[\u0027REVENUE_DATE\u0027] \u003e\u003d period_start_date].copy()\n\n    all_customer_names \u003d list(df[\u0027CUSTOMER\u0027].unique())\n\n    dd_find \u003d df[df[\u0027CUSTOMER\u0027].isin(all_customer_names)].copy()\n\n    consistent_customers_dd \u003d find_consistent_cust(dd_find, consecutive\u003dconsistency)\n    if len(consistent_customers_dd) \u003d\u003d 0:\n        continue\n\n    dd_find \u003d dd_find[dd_find[\u0027CUSTOMER\u0027].isin(consistent_customers_dd)].copy()\n    dd_find \u003d add_padding_func(dd_find, padding\u003dstatistics_period, last_date\u003dperiod_end_date)\n    dd_find \u003d find_average_func(dd_find, n\u003dstatistics_period)\n\n    dd_find[\u0027DD_INDICATOR\u0027] \u003d np.where(((drawdown*(dd_find[\u0027LAST_N_MONTHS_AVG\u0027].round(3)) \u003e\n                                     dd_find[\u0027ACTIVE_CARD_COUNT\u0027].round(3)) \u0026\n                                    (dd_find[\u0027NEXT_N_MONTHS_AVG\u0027].round(3) \u003c\n                                     drawdown_fwd_check*dd_find[\u0027LAST_N_MONTHS_AVG\u0027].round(3))),\n                                   True, False)\n\n    ## Find the first drawdown and also the list of customers\n    pflip_dd \u003d dd_find[dd_find[\u0027DD_INDICATOR\u0027] \u003d\u003d True].copy()\n    pflip_dd.drop_duplicates(\u0027CUSTOMER\u0027, inplace\u003dTrue)\n    first_drop_idx \u003d pflip_dd.index\n    pflip_dd_customers \u003d list(dd_find[\u0027CUSTOMER\u0027].unique())\n    first_drop \u003d dd_find.iloc[first_drop_idx]\n\n    ## Identify the lookback period\n    first_drop \u003d first_drop[[\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027]].copy()\n    first_drop \u003d first_drop[first_drop[\u0027REVENUE_DATE\u0027] \u003c\u003d inactive_date_start].copy()\n    first_drop[\u0027START_DATE\u0027]  \u003d first_drop[\u0027REVENUE_DATE\u0027] - pd.DateOffset(months\u003ddrawdown_lookback_period)\n    first_drop.rename(columns \u003d {\u0027REVENUE_DATE\u0027:\u0027DD_DATE\u0027}, inplace\u003dTrue)\n    dd_find_df \u003d dd_find[dd_find[\u0027CUSTOMER\u0027].isin(pflip_dd_customers)]\n    dd_find_df \u003d dd_find_df.merge(first_drop, on\u003d[\u0027CUSTOMER\u0027])\n    dd_find_df \u003d dd_find_df[dd_find_df[\u0027REVENUE_DATE\u0027].between(dd_find_df[\u0027START_DATE\u0027],dd_find_df[\u0027DD_DATE\u0027])].copy()\n\n    ## Compute the sharpest fall from the lookback period\n    dd_find_df.sort_values([\u0027CUSTOMER\u0027, \u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n    dd_find_df[\u0027DROP\u0027] \u003d dd_find_df.groupby([\u0027CUSTOMER\u0027])[\u0027ACTIVE_CARD_COUNT\u0027].diff(-1)\n\n    ## Find the corresponding period and remove duplicates in case of a similar values\n    drop_idx \u003d dd_find_df.groupby([\u0027CUSTOMER\u0027])[\u0027DROP\u0027].transform(max) \u003d\u003d dd_find_df[\u0027DROP\u0027]\n    drop_month_df \u003d dd_find_df[drop_idx].copy()\n    drop_month_df.drop_duplicates([\u0027CUSTOMER\u0027], inplace\u003dTrue)\n\n    ## remove the first record\n    dd_find \u003d dd_find.groupby(\u0027CUSTOMER\u0027).apply(lambda group: group.iloc[1:, 1:]).reset_index()\n    dd_find.drop(\u0027level_1\u0027, axis\u003d1, inplace\u003dTrue)\n\n    ## Find the time periods for calculating statistics (mean and standard deviation)\n    drop_month_df.rename(columns \u003d {\u0027REVENUE_DATE\u0027:\u0027DROP_DATE\u0027}, inplace\u003dTrue)\n    dd_find \u003d dd_find.merge(drop_month_df[[\u0027CUSTOMER\u0027, \u0027DROP_DATE\u0027]], on\u003d\u0027CUSTOMER\u0027)\n    dd_find[\u0027END_DATE\u0027] \u003d dd_find[\u0027DROP_DATE\u0027] - pd.DateOffset(months\u003d3)\n    dd_find[\u0027START_DATE\u0027] \u003d dd_find[\u0027END_DATE\u0027] - pd.DateOffset(months\u003dstatistics_period-1)\n    pflip_12_data \u003d dd_find[dd_find[\u0027REVENUE_DATE\u0027].between(dd_find[\u0027START_DATE\u0027], dd_find[\u0027END_DATE\u0027])].copy()\n\n    ## Calculate Mean and Standard Deviation\n    dd_stat \u003d pflip_12_data.groupby([\u0027CUSTOMER\u0027], as_index\u003dFalse).agg({\u0027ACTIVE_CARD_COUNT\u0027:[\u0027mean\u0027,\u0027std\u0027]})\n    dd_stat.columns \u003d [\u0027CUSTOMER_DD\u0027, \u0027MEAN_DD\u0027,\u0027STD_DD\u0027]\n    drop_month_df \u003d drop_month_df.merge(dd_stat,\n                                        left_on\u003d\u0027CUSTOMER\u0027,\n                                        right_on\u003d\u0027CUSTOMER_DD\u0027,\n                                        how\u003d\u0027left\u0027)\n\n    drop_month_df \u003d pd.merge(drop_month_df, df_max, on\u003d\u0027CUSTOMER\u0027, how\u003d\u0027left\u0027)\n    drop_month_df \u003d drop_month_df[[\u0027CUSTOMER\u0027,\u0027DROP_DATE\u0027,\u0027ACTIVE_CARD_MAX\u0027]]\n    drop_month_df.columns \u003d [\u0027CUSTOMER\u0027,\u0027DRAW_DOWN_DATE\u0027,\u0027ACTIVE_CARD_MAX\u0027]\n\n    print(len(drop_month_df), \"new drop records\")\n    drop_df \u003d pd.concat([drop_df, drop_month_df], ignore_index\u003dTrue)\n\n    print(len(drop_df), \"total drop records\")\n    print(\"saving to snowflake...\")\n    CALCULATED_DRAW_DOWNS_df \u003d drop_df\n    CALCULATED_DRAW_DOWNS \u003d dataiku.Dataset(\"CALCULATED_DRAW_DOWNS\")\n    CALCULATED_DRAW_DOWNS.write_with_schema(CALCULATED_DRAW_DOWNS_df)\n\n    pages_remaining \u003d total_pages-current_page\n\n    t1 \u003d time.time()\n    avg_duration \u003d (((t1-t0)/current_page)/60.0)\n    print(round(avg_duration,2), \"avg mins per iteration\")\n    print(round(pages_remaining,2), \"pages remaining\")\n    print(round(avg_duration*pages_remaining,2), \"estimated minutes remaining\")\n    print()\n\n    #\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\n    idx+\u003dpage_size\n\n    if max_pages\u003e0:\n        if current_page\u003e\u003dmax_pages:\n            break;\n\nprint(\"done\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "page 1\nprocessing 50000 customers\n1890955 data frame records\n13087 new drop records\n13087 total drop records\nsaving to snowflake...\n13087 rows successfully written (fEBUUjJheP)\n4.26 avg mins per iteration\n11.7 pages remaining\n49.84 estimated minutes remaining\n\npage 2\nprocessing 50000 customers\n1890919 data frame records\n13234 new drop records\n26321 total drop records\nsaving to snowflake...\n26321 rows successfully written (W1t5uuF1Y8)\n4.26 avg mins per iteration\n10.7 pages remaining\n45.58 estimated minutes remaining\n\npage 3\nprocessing 50000 customers\n1893134 data frame records\n13202 new drop records\n39523 total drop records\nsaving to snowflake...\n39523 rows successfully written (zpyffbOnbf)\n4.33 avg mins per iteration\n9.7 pages remaining\n42.01 estimated minutes remaining\n\npage 4\nprocessing 50000 customers\n1889736 data frame records\n13130 new drop records\n52653 total drop records\nsaving to snowflake...\n52653 rows successfully written (utIHvDiMgi)\n4.33 avg mins per iteration\n8.7 pages remaining\n37.69 estimated minutes remaining\n\npage 5\nprocessing 50000 customers\n1887931 data frame records\n13313 new drop records\n65966 total drop records\nsaving to snowflake...\n65966 rows successfully written (zaMFzJbxFl)\n4.35 avg mins per iteration\n7.7 pages remaining\n33.5 estimated minutes remaining\n\npage 6\nprocessing 50000 customers\n1805787 data frame records\n14451 new drop records\n80417 total drop records\nsaving to snowflake...\n80417 rows successfully written (oH6bdVfAiR)\n4.38 avg mins per iteration\n6.7 pages remaining\n29.34 estimated minutes remaining\n\npage 7\nprocessing 50000 customers\n1586079 data frame records\n17649 new drop records\n98066 total drop records\nsaving to snowflake...\n98066 rows successfully written (2JBJOJIOhU)\n4.39 avg mins per iteration\n5.7 pages remaining\n25.01 estimated minutes remaining\n\npage 8\nprocessing 50000 customers\n1264606 data frame records\n19369 new drop records\n117435 total drop records\nsaving to snowflake...\n117435 rows successfully written (Zt2buaCDEd)\n4.35 avg mins per iteration\n4.7 pages remaining\n20.45 estimated minutes remaining\n\npage 9\nprocessing 50000 customers\n1170565 data frame records\n15434 new drop records\n132869 total drop records\nsaving to snowflake...\n132869 rows successfully written (CiQZ9Nan4t)\n4.33 avg mins per iteration\n3.7 pages remaining\n16.0 estimated minutes remaining\n\npage 10\nprocessing 50000 customers\n740536 data frame records\n19901 new drop records\n152770 total drop records\nsaving to snowflake...\n152770 rows successfully written (zp9dUdDJN2)\n4.28 avg mins per iteration\n2.7 pages remaining\n11.53 estimated minutes remaining\n\npage 11\nprocessing 50000 customers\n504207 data frame records\n19678 new drop records\n172448 total drop records\nsaving to snowflake...\n172448 rows successfully written (7MK51tOsvz)\n4.22 avg mins per iteration\n1.7 pages remaining\n7.16 estimated minutes remaining\n\npage 12\nprocessing 50000 customers\n270709 data frame records\n6653 new drop records\n179101 total drop records\nsaving to snowflake...\n179101 rows successfully written (RFAItCA6QQ)\n4.15 avg mins per iteration\n0.7 pages remaining\n2.89 estimated minutes remaining\n\npage 13\nprocessing 34802 customers\n72183 data frame records\npage 14\nprocessing 34802 customers\n72183 data frame records\npage 15\nprocessing 34802 customers\n72183 data frame records\npage 16\nprocessing 34802 customers\n72183 data frame records\npage 17\nprocessing 34802 customers\n72183 data frame records\npage 18\nprocessing 34802 customers\n72183 data frame records\npage 19\nprocessing 34802 customers\n72183 data frame records\npage 20\nprocessing 34802 customers\n72183 data frame records\npage 21\nprocessing 34802 customers\n72183 data frame records\npage 22\nprocessing 34802 customers\n72183 data frame records\npage 23\nprocessing 34802 customers\n72183 data frame records\npage 24\nprocessing 34802 customers\n72183 data frame records\npage 25\nprocessing 34802 customers\n72183 data frame records\npage 26\nprocessing 34802 customers\n72183 data frame records\npage 27\nprocessing 34802 customers\n72183 data frame records\npage 28\nprocessing 34802 customers\n72183 data frame records\npage 29\nprocessing 34802 customers\n72183 data frame records\npage 30\nprocessing 34802 customers\n72183 data frame records\npage 31\nprocessing 34802 customers\n72183 data frame records\npage 32\nprocessing 34802 customers\n72183 data frame records\npage 33\nprocessing 34802 customers\n72183 data frame records\npage 34\nprocessing 34802 customers\n72183 data frame records\npage 35\nprocessing 34802 customers\n72183 data frame records\npage 36\nprocessing 34802 customers\n72183 data frame records\npage 37\nprocessing 34802 customers\n72183 data frame records\npage 38\nprocessing 34802 customers\n72183 data frame records\npage 39\nprocessing 34802 customers\n72183 data frame records\npage 40\nprocessing 34802 customers\n72183 data frame records\npage 41\nprocessing 34802 customers\n72183 data frame records\npage 42\nprocessing 34802 customers\n72183 data frame records\npage 43\nprocessing 34802 customers\n72183 data frame records\npage 44\nprocessing 34802 customers\n72183 data frame records\npage 45\n",
          "name": "stdout"
        }
      ]
    }
  ]
}