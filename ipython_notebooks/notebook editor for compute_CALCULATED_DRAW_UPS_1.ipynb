{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-env_clc",
      "display_name": "Python (env env_clc)",
      "language": "python"
    },
    "associatedRecipe": "compute_CALCULATED_DRAW_UPS",
    "creator": "Daniel.Vandermeer",
    "createdOn": 1667145106521,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {},
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "Daniel.Vandermeer"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 24,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\nimport time\n\nimport pickle\nfrom dateutil.relativedelta import relativedelta\nimport gc\nfrom re import finditer\n\n## Find DD DU\nfrom helper import preprocess_data\nfrom patterns import find_drawdowns, find_drawups\n\n## MATCHING\nimport name_matching\nfrom name_matching import name_match\nimport transaction_matching\nfrom transaction_matching import transaction_match\n\n## CONSOLIDATION\nfrom consolidation import combine_matches, consolidate_matches, find_attritions, find_new_accounts, get_attrition_status, get_new_account_status\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 25,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "start_date \u003d dataiku.get_custom_variables()[\u0027start_date\u0027]\nend_date \u003d dataiku.get_custom_variables()[\u0027end_date\u0027]\n\nconsistency \u003d int(dataiku.get_custom_variables()[\u0027consistency\u0027])\ndrawdown_period_average \u003d int(dataiku.get_custom_variables()[\u0027drawdown_period_average\u0027])\ndrawdown \u003d int(dataiku.get_custom_variables()[\u0027drawdown\u0027])\ndrawdown_fwd_check \u003d int(dataiku.get_custom_variables()[\u0027drawdown_fwd_check\u0027])\ndrawdown_lookback_period \u003d int(dataiku.get_custom_variables()[\u0027drawdown_lookback_period\u0027])\ndrawup_lookfwd_period \u003d int(dataiku.get_custom_variables()[\u0027drawup_lookfwd_period\u0027])\nstatistics_period \u003d int(dataiku.get_custom_variables()[\u0027statistics_period\u0027])\ninactive_period \u003d int(dataiku.get_custom_variables()[\u0027inactive_period\u0027])\n\n## MATCHING VARIABLES\nmonth_diff_h \u003d int(dataiku.get_custom_variables()[\u0027month_diff_h\u0027])\nmonth_diff_l \u003d int(dataiku.get_custom_variables()[\u0027month_diff_l\u0027])\nsd_mul \u003d int(dataiku.get_custom_variables()[\u0027sd_mul\u0027])\nmax_city_distance \u003d int(dataiku.get_custom_variables()[\u0027max_city_distance\u0027])\nthreshold_score_step1 \u003d int(dataiku.get_custom_variables()[\u0027threshold_score_step1\u0027])\nthreshold_score_step2 \u003d int(dataiku.get_custom_variables()[\u0027threshold_score_step2\u0027])\n\n## RUN TYPE\nrun \u003d dataiku.get_custom_variables()[\u0027run_type\u0027]\n\nprint(\"start_date\", start_date)\nprint(\"end_date\", end_date)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "start_date 2019-01-01\nend_date 2022-10-01\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 26,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read recipe inputs\nNAFCUSTOMER_ACTIVE_CARDS_FULL \u003d dataiku.Dataset(\"NAFCUSTOMER_ACTIVE_CARDS_FULL\")\nNAFCUSTOMER_ACTIVE_CARDS_FULL_df \u003d NAFCUSTOMER_ACTIVE_CARDS_FULL.get_dataframe()\nprint(len(NAFCUSTOMER_ACTIVE_CARDS_FULL_df))\nNAFCUSTOMER_ACTIVE_CARDS_FULL_df.head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "16564685\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "                              CUSTOMER  REVENUE_YEAR  REVENUE_MONTH  ACTIVE_CARD_COUNT\n0                   547121 ONT LTD O/A          2019              1               16.0\n1        PARTNERS IN MISSION FOOD BANK          2019              1                1.0\n2       BEKINS A 1 MOVERS INC. (CHICAG          2019              1               10.0\n3                B LEVEL SOUTHEAST INC          2019              1                2.0\n4  COLDSMITHS CONSTRUCTION COMPANY INC          2019              1               11.0",
            "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style\u003d\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003eCUSTOMER\u003c/th\u003e\n      \u003cth\u003eREVENUE_YEAR\u003c/th\u003e\n      \u003cth\u003eREVENUE_MONTH\u003c/th\u003e\n      \u003cth\u003eACTIVE_CARD_COUNT\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003e547121 ONT LTD O/A\u003c/td\u003e\n      \u003ctd\u003e2019\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e16.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003ePARTNERS IN MISSION FOOD BANK\u003c/td\u003e\n      \u003ctd\u003e2019\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e1.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003eBEKINS A 1 MOVERS INC. (CHICAG\u003c/td\u003e\n      \u003ctd\u003e2019\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e10.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003eB LEVEL SOUTHEAST INC\u003c/td\u003e\n      \u003ctd\u003e2019\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e2.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003eCOLDSMITHS CONSTRUCTION COMPANY INC\u003c/td\u003e\n      \u003ctd\u003e2019\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e11.0\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          },
          "metadata": {}
        }
      ]
    },
    {
      "execution_count": 12,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def date_tz_naive(pd_s):\n    return pd.to_datetime(pd_s).apply(lambda x:x.tz_localize(None))"
      ],
      "outputs": []
    },
    {
      "execution_count": 13,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "customer_list_full \u003d NAFCUSTOMER_ACTIVE_CARDS_FULL_df.CUSTOMER.unique()\nprint(len(customer_list_full))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "624069\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 23,
      "cell_type": "code",
      "metadata": {
        "scrolled": false
      },
      "source": [
        "page_size \u003d 50000\nidx \u003d 0\ncurrent_page \u003d 0\nmax_pages \u003d 0\n\ndrop_df \u003d pd.DataFrame()\nt0 \u003d time.time()\n\ntotal_pages \u003d len(customer_list_full)/page_size\n\nrise_df \u003d pd.DataFrame()\n\nwhile idx\u003clen(customer_list_full):\n    \n    current_page+\u003d1\n    print(\"page\", current_page)\n    \n    to_range \u003d idx+page_size\n    if to_range\u003elen(customer_list_full):\n        to_range \u003d len(customer_list_full)-1\n        \n    current_set \u003d customer_list_full[idx:to_range]\n    \n    #\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n    \n    df_v \u003d NAFCUSTOMER_ACTIVE_CARDS_FULL_df[NAFCUSTOMER_ACTIVE_CARDS_FULL_df.CUSTOMER.isin(current_set)]\n    print(\"processing\", len(df_v.CUSTOMER.unique()), \"customers\")\n    print(len(df_v), \"data frame records\")\n    \n    df_v[\u0027REVENUE_DATE\u0027] \u003d df_v.REVENUE_MONTH.astype(str) + \"/01/\" + df_v.REVENUE_YEAR.astype(str)\n    df_v[\u0027REVENUE_DATE\u0027] \u003d date_tz_naive(df_v[\u0027REVENUE_DATE\u0027])\n    \n    df_v \u003d df_v[df_v[\u0027REVENUE_DATE\u0027].between(pd.to_datetime(start_date), pd.to_datetime(end_date))].copy()\n    df_v \u003d df_v.dropna(subset\u003d[\u0027CUSTOMER\u0027])\n    \n    df_v[\u0027REVENUE_DATE\u0027] \u003d pd.to_datetime(df_v[\u0027REVENUE_DATE\u0027])\n    df_v \u003d df_v[[\u0027CUSTOMER\u0027,\u0027REVENUE_DATE\u0027, \u0027ACTIVE_CARD_COUNT\u0027]]\n\n    df_v_max \u003d df_v[[\u0027CUSTOMER\u0027,\u0027ACTIVE_CARD_COUNT\u0027]]\n    df_max \u003d df_v_max.groupby(by\u003d[\"CUSTOMER\"]).max().reset_index()\n    df_max.columns \u003d [\u0027CUSTOMER\u0027, \u0027ACTIVE_CARD_MAX\u0027]\n    \n    match_type \u003d \"program_flip\"\n    period_start_date \u003d start_date\n    period_end_date \u003d None\n    drawup_window \u003d drawup_lookfwd_period\n    statistics_period \u003d statistics_period\n    split \u003d None\n    \n    period_start_date \u003d pd.to_datetime(period_start_date)\n    df_v \u003d df_v[df_v[\u0027REVENUE_DATE\u0027] \u003e\u003d period_start_date].copy()\n\n    if period_end_date:\n        period_end_date \u003d pd.to_datetime(period_end_date)\n        df_v \u003d df_v[df_v[\u0027revenue_date\u0027] \u003c\u003d period_end_date].copy()\n\n    all_customer_names \u003d list(df_v[\u0027CUSTOMER\u0027].unique())\n\n    du_find \u003d df_v[df_v[\u0027CUSTOMER\u0027].isin(all_customer_names)].copy()\n\n    ## Filter Non-Zero Records and find the first non zero transaction date\n    du_find \u003d du_find[du_find[\u0027ACTIVE_CARD_COUNT\u0027] \u003e 0]\n\n    du_find.sort_values([\u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n\n    du_agg \u003d du_find.groupby([\u0027CUSTOMER\u0027], as_index\u003dFalse)[[\u0027REVENUE_DATE\u0027]].min()\n\n    du_agg[\u0027DU_INDICATOR\u0027] \u003d np.where((du_agg[\u0027REVENUE_DATE\u0027] \u003e period_start_date), True, False)\n    du_agg.rename(columns\u003d{\u0027REVENUE_DATE\u0027:\u0027DU_DATE\u0027}, inplace\u003dTrue)\n    du_agg[\u0027DU_DATE\u0027] -\u003d pd.DateOffset(months\u003d1)\n    du_agg \u003d du_agg[du_agg[\u0027DU_INDICATOR\u0027] \u003d\u003d True].drop_duplicates([\u0027CUSTOMER\u0027])\n\n    ## list of customers who are drawing up\n    du_customers \u003d list(du_agg[\u0027CUSTOMER\u0027])\n\n    if len(du_customers) \u003d\u003d 0:\n        continue\n\n    du_find \u003d du_find[du_find[\u0027CUSTOMER\u0027].isin(du_customers)].copy()\n\n    du_find \u003d du_find.groupby(\u0027CUSTOMER\u0027).apply(lambda group: group.iloc[:-1, 1:]).reset_index()\n    du_find.drop(\u0027level_1\u0027, axis\u003d1, inplace\u003dTrue)\n\n    du_find \u003d du_find.merge(du_agg, left_on\u003d[\u0027CUSTOMER\u0027], right_on\u003d[\u0027CUSTOMER\u0027])\n\n    du_find[\u0027DU_AVG_START\u0027] \u003d du_find[\u0027DU_DATE\u0027]  + pd.DateOffset(months\u003ddrawup_window)\n    du_find[\u0027DU_AVG_END\u0027] \u003d du_find[\u0027DU_DATE\u0027]  + pd.DateOffset(months\u003ddrawup_window+statistics_period-1)\n\n    du_find_12 \u003d du_find[du_find[\u0027REVENUE_DATE\u0027].between(du_find[\u0027DU_AVG_START\u0027], du_find[\u0027DU_AVG_END\u0027])].copy()\n\n    du_stat \u003d du_find_12.groupby([\u0027CUSTOMER\u0027], as_index\u003dFalse).agg({\u0027ACTIVE_CARD_COUNT\u0027:[\u0027mean\u0027,\u0027std\u0027]})\n\n    du_stat.columns \u003d [\u0027CUSTOMER\u0027, \u0027mean_du\u0027,\u0027std_du\u0027]\n\n    rise_df_ \u003d du_agg.merge(du_stat, left_on\u003d\u0027CUSTOMER\u0027, right_on\u003d\u0027CUSTOMER\u0027, how\u003d\u0027left\u0027)\n    \n    rise_df_ \u003d pd.merge(rise_df_, df_max, on\u003d\u0027CUSTOMER\u0027, how\u003d\u0027left\u0027)\n    df_max\n    \n    print(len(rise_df_), \"new rise records\")\n    rise_df \u003d pd.concat([rise_df, rise_df_], ignore_index\u003dTrue)\n    \n    print(len(rise_df), \"total rise records\")\n    print(\"saving to snowflake...\")\n    \n    CALCULATED_DRAW_UPS_df \u003d rise_df\n    CALCULATED_DRAW_UPS \u003d dataiku.Dataset(\"CALCULATED_DRAW_UPS\")\n    CALCULATED_DRAW_UPS.write_with_schema(CALCULATED_DRAW_UPS_df)\n    \n    #\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n    \n    pages_remaining \u003d total_pages-current_page\n    \n    t1 \u003d time.time()\n    avg_duration \u003d (((t1-t0)/current_page)/60.0)\n    print(round(avg_duration,2), \"avg mins per iteration\")\n    print(round(pages_remaining,0), \"pages remaining\")\n    print(round(avg_duration*pages_remaining,2), \"estimated minutes remaining\")\n    print()\n    \n    #\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n    \n    idx+\u003dpage_size\n    \n    if max_pages\u003e0:\n        if current_page\u003e\u003dmax_pages:\n            break;\n    "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "page 1\nprocessing 50000 customers\n1828930 data frame records\n1144 new rise records\n1144 total rise records\nsaving to snowflake...\n1144 rows successfully written (jtQcDt0p7f)\n0.52 avg mins per iteration\n11.0 pages remaining\n5.95 estimated minutes remaining\n\npage 2\nprocessing 50000 customers\n1824387 data frame records\n1135 new rise records\n2279 total rise records\nsaving to snowflake...\n2279 rows successfully written (h5E6U2EZm2)\n0.52 avg mins per iteration\n10.0 pages remaining\n5.43 estimated minutes remaining\n\npage 3\nprocessing 50000 customers\n1822621 data frame records\n1124 new rise records\n3403 total rise records\nsaving to snowflake...\n3403 rows successfully written (NleCYe6Zfb)\n0.51 avg mins per iteration\n9.0 pages remaining\n4.88 estimated minutes remaining\n\npage 4\nprocessing 50000 customers\n1823772 data frame records\n1164 new rise records\n4567 total rise records\nsaving to snowflake...\n4567 rows successfully written (q0cphwCxif)\n0.51 avg mins per iteration\n8.0 pages remaining\n4.36 estimated minutes remaining\n\npage 5\nprocessing 50000 customers\n1828254 data frame records\n1171 new rise records\n5738 total rise records\nsaving to snowflake...\n5738 rows successfully written (Orw3Lj2UaM)\n0.52 avg mins per iteration\n7.0 pages remaining\n3.86 estimated minutes remaining\n\npage 6\nprocessing 50000 customers\n1754247 data frame records\n19474 new rise records\n25212 total rise records\nsaving to snowflake...\n25212 rows successfully written (E10lZUipKZ)\n0.57 avg mins per iteration\n6.0 pages remaining\n3.72 estimated minutes remaining\n\npage 7\nprocessing 50000 customers\n1551872 data frame records\n49127 new rise records\n74339 total rise records\nsaving to snowflake...\n74339 rows successfully written (qSV2EIv7NF)\n0.7 avg mins per iteration\n5.0 pages remaining\n3.84 estimated minutes remaining\n\npage 8\nprocessing 50000 customers\n1253293 data frame records\n46390 new rise records\n120729 total rise records\nsaving to snowflake...\n120729 rows successfully written (zDpNMmul5q)\n0.79 avg mins per iteration\n4.0 pages remaining\n3.56 estimated minutes remaining\n\npage 9\nprocessing 50000 customers\n1170698 data frame records\n47788 new rise records\n168517 total rise records\nsaving to snowflake...\n168517 rows successfully written (MiUpU7yj5f)\n0.89 avg mins per iteration\n3.0 pages remaining\n3.1 estimated minutes remaining\n\npage 10\nprocessing 50000 customers\n762083 data frame records\n47410 new rise records\n215927 total rise records\nsaving to snowflake...\n215927 rows successfully written (KK56m6nSkC)\n0.97 avg mins per iteration\n2.0 pages remaining\n2.4 estimated minutes remaining\n\npage 11\nprocessing 50000 customers\n519218 data frame records\n47683 new rise records\n263610 total rise records\nsaving to snowflake...\n263610 rows successfully written (KZzhE2KUIo)\n1.04 avg mins per iteration\n1.0 pages remaining\n1.54 estimated minutes remaining\n\npage 12\nprocessing 50000 customers\n305364 data frame records\n44419 new rise records\n308029 total rise records\nsaving to snowflake...\n308029 rows successfully written (t3ld8GLTcB)\n1.09 avg mins per iteration\n0.0 pages remaining\n0.53 estimated minutes remaining\n\npage 13\nprocessing 24068 customers\n60788 data frame records\n16855 new rise records\n324884 total rise records\nsaving to snowflake...\n324884 rows successfully written (NoxfgZe89C)\n1.11 avg mins per iteration\n-1.0 pages remaining\n-0.58 estimated minutes remaining\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}