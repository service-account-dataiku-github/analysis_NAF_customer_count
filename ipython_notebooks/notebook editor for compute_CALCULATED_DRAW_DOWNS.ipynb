{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-env_clc",
      "display_name": "Python (env env_clc)",
      "language": "python"
    },
    "associatedRecipe": "compute_CALCULATED_DRAW_DOWNS",
    "dkuGit": {
      "lastInteraction": 0
    },
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "Daniel.Vandermeer"
      },
      "lastModifiedOn": 1665593021198
    },
    "creator": "Daniel.Vandermeer",
    "createdOn": 1665593021198,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {},
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "Daniel.Vandermeer"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 53,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\nimport pickle\nfrom dateutil.relativedelta import relativedelta\nimport gc\nfrom re import finditer\n\n## Find DD DU\nfrom helper import preprocess_data\nfrom patterns import find_drawdowns, find_drawups\n\n## MATCHING\nimport name_matching\nfrom name_matching import name_match\nimport transaction_matching\nfrom transaction_matching import transaction_match\n\n## CONSOLIDATION\nfrom consolidation import combine_matches, consolidate_matches, find_attritions, find_new_accounts, get_attrition_status, get_new_account_status"
      ],
      "outputs": []
    },
    {
      "execution_count": 54,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "start_date \u003d dataiku.get_custom_variables()[\u0027start_date\u0027]\nend_date \u003d dataiku.get_custom_variables()[\u0027end_date\u0027]\n\nconsistency \u003d int(dataiku.get_custom_variables()[\u0027consistency\u0027])\ndrawdown_period_average \u003d int(dataiku.get_custom_variables()[\u0027drawdown_period_average\u0027])\ndrawdown \u003d int(dataiku.get_custom_variables()[\u0027drawdown\u0027])\ndrawdown_fwd_check \u003d int(dataiku.get_custom_variables()[\u0027drawdown_fwd_check\u0027])\ndrawdown_lookback_period \u003d int(dataiku.get_custom_variables()[\u0027drawdown_lookback_period\u0027])\ndrawup_lookfwd_period \u003d int(dataiku.get_custom_variables()[\u0027drawup_lookfwd_period\u0027])\nstatistics_period \u003d int(dataiku.get_custom_variables()[\u0027statistics_period\u0027])\ninactive_period \u003d int(dataiku.get_custom_variables()[\u0027inactive_period\u0027])\n\n## MATCHING VARIABLES\nmonth_diff_h \u003d int(dataiku.get_custom_variables()[\u0027month_diff_h\u0027])\nmonth_diff_l \u003d int(dataiku.get_custom_variables()[\u0027month_diff_l\u0027])\nsd_mul \u003d int(dataiku.get_custom_variables()[\u0027sd_mul\u0027])\nmax_city_distance \u003d int(dataiku.get_custom_variables()[\u0027max_city_distance\u0027])\nthreshold_score_step1 \u003d int(dataiku.get_custom_variables()[\u0027threshold_score_step1\u0027])\nthreshold_score_step2 \u003d int(dataiku.get_custom_variables()[\u0027threshold_score_step2\u0027])\n\n## RUN TYPE\nrun \u003d dataiku.get_custom_variables()[\u0027run_type\u0027]"
      ],
      "outputs": []
    },
    {
      "execution_count": 55,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def date_tz_naive(pd_s):\n    return pd.to_datetime(pd_s).apply(lambda x:x.tz_localize(None))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read recipe inputs\nNAFCUSTOMER_C360_ACCOUNTS \u003d dataiku.Dataset(\"NAFCUSTOMER_C360_ACCOUNTS\")\nNAFCUSTOMER_C360_ACCOUNTS_df \u003d NAFCUSTOMER_C360_ACCOUNTS.get_dataframe()\nprint(len(NAFCUSTOMER_C360_ACCOUNTS_df))\n\nNAFCUSTOMER_REVENUE_BY_CUSTOMER_LIMITED \u003d dataiku.Dataset(\"NAFCUSTOMER_REVENUE_BY_CUSTOMER_LIMITED\")\nNAFCUSTOMER_REVENUE_BY_CUSTOMER_LIMITED_df \u003d NAFCUSTOMER_REVENUE_BY_CUSTOMER_LIMITED.get_dataframe()\nprint(len(NAFCUSTOMER_REVENUE_BY_CUSTOMER_LIMITED_df))\n\nprint(NAFCUSTOMER_REVENUE_BY_CUSTOMER_LIMITED_df.CUSTOMER_ACCOUNT_ID.unique(), \"accounts\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "844417\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_v \u003d NAFCUSTOMER_REVENUE_BY_CUSTOMER_LIMITED_df\n\nprint(len(df_v))\ndf_v[\u0027REVENUE_DATE\u0027] \u003d df_v.REVENUE_MONTH.astype(str) + \"/01/\" + df_v.REVENUE_YEAR.astype(str)\ndf_v[\u0027REVENUE_DATE\u0027] \u003d date_tz_naive(df_v[\u0027REVENUE_DATE\u0027])\nprint(len(df_v))\ndf_v.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(len(df_v))\ndf_v \u003d df_v[df_v[\u0027REVENUE_DATE\u0027].between(pd.to_datetime(start_date), pd.to_datetime(end_date))].copy()\ndf_v \u003d df_v.dropna(subset\u003d[\u0027CUSTOMER_ACCOUNT_ID\u0027])\ndf_v \u003d df_v[df_v[\u0027CUSTOMER_SOURCE_SYSTEM_CODE\u0027].isin([\u0027TANDEM\u0027, \u0027SIEBEL\u0027])]\nprint(len(df_v))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_v[\u0027CUSTOMER_ACCOUNT_ID\u0027] \u003d df_v[\u0027CUSTOMER_ACCOUNT_ID\u0027].astype(\u0027int64\u0027)\ndf_v[\u0027REVENUE_DATE\u0027] \u003d pd.to_datetime(df_v[\u0027REVENUE_DATE\u0027])"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "states \u003d list(df_v[\u0027ACCOUNT_STATE\u0027].unique())\nstates_dict \u003d {s:s.upper() for s in states}"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_v[\u0027ACCOUNT_STATE\u0027] \u003d df_v[\u0027ACCOUNT_STATE\u0027].map(states_dict)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## remove the unneccesary columns\nremove_cols\u003d[\u0027REVENUE_MONTH\u0027,\u0027REVENUE_YEAR\u0027, \u0027REVENUE_QUARTER\u0027]\ndf_v \u003d df_v.drop([x for x in remove_cols if x in df_v.columns], axis\u003d1)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_v.sort_values([\u0027REVENUE_DATE\u0027], inplace\u003dTrue)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "seen_accounts \u003d df_v[df_v[\u0027PURCHASE_GALLONS_QTY\u0027] \u003e 0].groupby([\u0027CUSTOMER_ACCOUNT_ID\u0027], as_index\u003dFalse)[[\u0027REVENUE_DATE\u0027]].first()\nseen_accounts[\u0027FIRST_DATE\u0027] \u003d seen_accounts[\u0027REVENUE_DATE\u0027] - pd.DateOffset(months\u003d1)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_v.REVENUE_DATE.value_counts(dropna\u003dFalse)\nprint(len(df_v))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom dateutil.relativedelta import relativedelta\nfrom helper import *\n\n#---------------------\n# input vars\ndf \u003d df_v\nperiod_end_date \u003d end_date\nmatch_type \u003d \u0027program_flip\u0027\nperiod_start_date\u003dNone\nsplit\u003dNone\n#------------------------\n\ndrawdown \u003d (100 - drawdown)/100\ndrawdown_fwd_check /\u003d 100\n\ninactive_date_start \u003d pd.to_datetime(period_end_date) + relativedelta(months\u003d-inactive_period)\n\nif match_type \u003d\u003d \u0027conversion\u0027:\n    df \u003d df[df[\u0027CUSTOMER_SOURCE_SYSTEM_CODE\u0027] \u003d\u003d \u0027TANDEM\u0027].copy()\n\ndf \u003d df[df[\u0027REVENUE_DATE\u0027] \u003c\u003d period_end_date].copy()\n\nif period_start_date:\n    period_start_date \u003d pd.to_datetime(period_start_date)\n    df \u003d df[df[\u0027REVENUE_DATE\u0027] \u003e\u003d period_start_date].copy()\n\nall_account_ids \u003d list(df[\u0027CUSTOMER_ACCOUNT_ID\u0027].unique())\n\nif not split:\n    split\u003d1\n\nall_account_ids_n \u003d list(split_list(all_account_ids, split))\n\ndrop_df \u003d pd.DataFrame()\n\nfor sublist in tqdm(all_account_ids_n):\n\n    dd_find \u003d df[df[\u0027CUSTOMER_ACCOUNT_ID\u0027].isin(sublist)].copy()\n\n    ## Find consistent customers\n    consistent_customers_dd \u003d find_consistent_customers(dd_find, consecutive\u003dconsistency)\n    if len(consistent_customers_dd) \u003d\u003d 0:\n        continue\n\n    dd_find \u003d dd_find[dd_find[\u0027CUSTOMER_ACCOUNT_ID\u0027].isin(consistent_customers_dd)].copy()\n\n    ## Add padding, find the n months average and compute drawdown indicator based on the rules\n    dd_find \u003d add_padding(dd_find, padding\u003dstatistics_period, last_date\u003dperiod_end_date)\n    dd_find \u003d find_average(dd_find, n\u003dstatistics_period)\n\n    dd_find[\u0027DD_INDICATOR\u0027] \u003d np.where(((drawdown*(dd_find[\u0027LAST_N_MONTHS_AVG\u0027].round(3)) \u003e\n                                         dd_find[\u0027PURCHASE_GALLONS_QTY\u0027].round(3)) \u0026\n                                        (dd_find[\u0027NEXT_N_MONTHS_AVG\u0027].round(3) \u003c\n                                         drawdown_fwd_check*dd_find[\u0027LAST_N_MONTHS_AVG\u0027].round(3))),\n                                       True, False)\n\n    ## Find the first drawdown and also the list of customers\n    pflip_dd \u003d dd_find[dd_find[\u0027DD_INDICATOR\u0027] \u003d\u003d True].copy()\n    pflip_dd.drop_duplicates(\u0027CUSTOMER_ACCOUNT_ID\u0027, inplace\u003dTrue)\n    first_drop_idx \u003d pflip_dd.index\n    pflip_dd_customers \u003d list(dd_find[\u0027CUSTOMER_ACCOUNT_ID\u0027].unique())\n    first_drop \u003d dd_find.iloc[first_drop_idx]\n\n    ## Identify the lookback period\n    first_drop \u003d first_drop[[\u0027CUSTOMER_ACCOUNT_ID\u0027, \u0027REVENUE_DATE\u0027]].copy()\n    first_drop \u003d first_drop[first_drop[\u0027REVENUE_DATE\u0027] \u003c\u003d inactive_date_start].copy()\n    first_drop[\u0027START_DATE\u0027]  \u003d first_drop[\u0027REVENUE_DATE\u0027] - pd.DateOffset(months\u003ddrawdown_lookback_period)\n    first_drop.rename(columns \u003d {\u0027REVENUE_DATE\u0027:\u0027DD_DATE\u0027}, inplace\u003dTrue)\n    dd_find_df \u003d dd_find[dd_find[\u0027CUSTOMER_ACCOUNT_ID\u0027].isin(pflip_dd_customers)]\n    dd_find_df \u003d dd_find_df.merge(first_drop, on\u003d[\u0027CUSTOMER_ACCOUNT_ID\u0027])\n    dd_find_df \u003d dd_find_df[dd_find_df[\u0027REVENUE_DATE\u0027].between(dd_find_df[\u0027START_DATE\u0027],dd_find_df[\u0027DD_DATE\u0027])].copy()\n\n    ## Compute the sharpest fall from the lookback period\n    dd_find_df.sort_values([\u0027CUSTOMER_ACCOUNT_ID\u0027, \u0027REVENUE_DATE\u0027], inplace\u003dTrue)\n    dd_find_df[\u0027DROP\u0027] \u003d dd_find_df.groupby([\u0027CUSTOMER_ACCOUNT_ID\u0027])[\u0027PURCHASE_GALLONS_QTY\u0027].diff(-1)\n\n    ## Find the corresponding period and remove duplicates in case of a similar values\n    drop_idx \u003d dd_find_df.groupby([\u0027CUSTOMER_ACCOUNT_ID\u0027])[\u0027DROP\u0027].transform(max) \u003d\u003d dd_find_df[\u0027DROP\u0027]\n    drop_month_df \u003d dd_find_df[drop_idx].copy()\n    drop_month_df.drop_duplicates([\u0027CUSTOMER_ACCOUNT_ID\u0027], inplace\u003dTrue)\n\n    ## remove the first record\n    dd_find \u003d dd_find.groupby(\u0027CUSTOMER_ACCOUNT_ID\u0027).apply(lambda group: group.iloc[1:, 1:]).reset_index()\n    dd_find.drop(\u0027level_1\u0027, axis\u003d1, inplace\u003dTrue)\n\n    ## Find the time periods for calculating statistics (mean and standard deviation)\n    drop_month_df.rename(columns \u003d {\u0027REVENUE_DATE\u0027:\u0027DROP_DATE\u0027}, inplace\u003dTrue)\n    dd_find \u003d dd_find.merge(drop_month_df[[\u0027CUSTOMER_ACCOUNT_ID\u0027, \u0027DROP_DATE\u0027]], on\u003d\u0027CUSTOMER_ACCOUNT_ID\u0027)\n    dd_find[\u0027END_DATE\u0027] \u003d dd_find[\u0027DROP_DATE\u0027] - pd.DateOffset(months\u003d3)\n    dd_find[\u0027START_DATE\u0027] \u003d dd_find[\u0027END_DATE\u0027] - pd.DateOffset(months\u003dstatistics_period-1)\n    pflip_12_data \u003d dd_find[dd_find[\u0027REVENUE_DATE\u0027].between(dd_find[\u0027START_DATE\u0027], dd_find[\u0027END_DATE\u0027])].copy()\n\n    ## Calculate Mean and Standard Deviation\n    dd_stat \u003d pflip_12_data.groupby([\u0027CUSTOMER_ACCOUNT_ID\u0027], as_index\u003dFalse).agg({\u0027PURCHASE_GALLONS_QTY\u0027:[\u0027mean\u0027,\u0027std\u0027]})\n    dd_stat.columns \u003d [\u0027CUSTOMER_ACCOUNT_ID_DD\u0027, \u0027MEAN_DD\u0027,\u0027STD_DD\u0027]\n    drop_month_df \u003d drop_month_df.merge(dd_stat,\n                                        left_on\u003d\u0027CUSTOMER_ACCOUNT_ID\u0027,\n                                        right_on\u003d\u0027CUSTOMER_ACCOUNT_ID_DD\u0027,\n                                        how\u003d\u0027left\u0027)\n\n    drop_df \u003d pd.concat([drop_df, drop_month_df], ignore_index\u003dTrue)\n\ndrop_df.drop([\u0027CUSTOMER_ACCOUNT_ID_DD\u0027], axis\u003d1, inplace\u003dTrue)\ndrop_df.rename(columns\u003d{\u0027CUSTOMER_ACCOUNT_ID\u0027:\u0027CUSTOMER_ACCOUNT_ID_DD\u0027,\n                        \u0027CUSTOMER_ACCOUNT_NAME\u0027: \u0027CUSTOMER_ACCOUNT_NAME_DD\u0027,\n                        \u0027CUSTOMER_NAME\u0027: \u0027CUSTOMER_NAME_DD\u0027}, inplace\u003dTrue)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "len(drop_df.CUSTOMER_ACCOUNT_ID_DD.unique())"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "drop_df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute recipe outputs from inputs\n# TODO: Replace this part by your actual code that computes the output, as a Pandas dataframe\n# NB: DSS also supports other kinds of APIs for reading and writing data. Please see doc.\n\n\nCALCULATED_DRAW_DOWNS_df \u003d drop_df\n\n\n# Write recipe outputs\nCALCULATED_DRAW_DOWNS \u003d dataiku.Dataset(\"CALCULATED_DRAW_DOWNS\")\nCALCULATED_DRAW_DOWNS.write_with_schema(CALCULATED_DRAW_DOWNS_df)"
      ],
      "outputs": []
    }
  ]
}